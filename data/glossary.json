[
  {
    "slug": "neural-network",
    "language": "en",
    "term": "Neural Network",
    "category": "Architecture",
    "pronunciation": "/ˈnjʊərəl ˈnetwɜːrk/",
    "definition": "A computational model inspired by biological neural networks, consisting of interconnected nodes (neurons) that process information.",
    "explanation": "A neural network is a machine learning model inspired by the human brain's structure. It consists of layers of interconnected nodes (neurons) that process and transform input data to produce output predictions.\n\n## Architecture\nNeural networks typically have:\n- **Input Layer**: Receives raw data\n- **Hidden Layers**: Process and transform data\n- **Output Layer**: Produces final predictions\n\n## Training\nNetworks learn through backpropagation, adjusting weights to minimize error between predictions and actual outputs.",
    "examples": ["Image classification", "Speech recognition", "Natural language processing"],
    "relatedTerms": ["deep-learning", "backpropagation", "activation-function"]
  },
  {
    "slug": "transformer",
    "language": "en",
    "term": "Transformer",
    "category": "Architecture",
    "pronunciation": "/trænsˈfɔːrmər/",
    "definition": "A neural network architecture based entirely on attention mechanisms, without recurrent or convolutional layers.",
    "explanation": "Transformers revolutionized NLP by processing entire sequences in parallel using self-attention. Key components include multi-head attention, positional encoding, and feed-forward networks. Models like GPT and BERT are based on this architecture.",
    "examples": ["GPT (Generative Pre-trained Transformer)", "BERT (Bidirectional Encoder Representations from Transformers)", "T5 (Text-to-Text Transfer Transformer)"],
    "relatedTerms": ["self-attention", "bert", "gpt"]
  },
  {
    "slug": "deep-learning",
    "language": "en",
    "term": "Deep Learning",
    "category": "Concept",
    "pronunciation": "/diːp ˈlɜːrnɪŋ/",
    "definition": "A subset of machine learning using neural networks with multiple layers to learn hierarchical representations of data.",
    "explanation": "Deep learning models automatically learn features from raw data through multiple layers of abstraction. Each layer learns increasingly complex representations, from simple edges to complex objects. Requires large datasets and computational power.",
    "examples": ["Image classification with CNNs", "Language models like GPT", "Speech recognition systems"],
    "relatedTerms": ["neural-network", "machine-learning", "backpropagation"]
  },
  {
    "slug": "gpt",
    "language": "en",
    "term": "GPT (Generative Pre-trained Transformer)",
    "category": "Model",
    "pronunciation": "/dʒiː piː tiː/",
    "definition": "A family of large language models developed by OpenAI that use transformer architecture for text generation.",
    "explanation": "GPT models are trained on massive text corpora using unsupervised learning, then fine-tuned for specific tasks. They excel at text generation, translation, summarization, and question-answering. GPT-4 is one of the most advanced language models.",
    "examples": ["ChatGPT for conversational AI", "Code generation with GitHub Copilot", "Creative writing assistance"],
    "relatedTerms": ["transformer", "llm", "bert"]
  },
  {
    "slug": "llm",
    "language": "en",
    "term": "LLM (Large Language Model)",
    "category": "Model",
    "pronunciation": "/ɛl ɛl ɛm/",
    "definition": "A neural network with billions of parameters trained on massive text datasets to understand and generate human language.",
    "explanation": "LLMs like GPT-4, Claude, and LLaMA are trained on diverse text from the internet. They demonstrate emergent abilities like reasoning, few-shot learning, and task generalization. Scale is key - larger models show better performance.",
    "examples": ["GPT-4 with 1.76 trillion parameters", "LLaMA 2 for open-source applications", "Claude for long-context understanding"],
    "relatedTerms": ["gpt", "transformer", "fine-tuning"]
  },
  {
    "slug": "backpropagation",
    "language": "en",
    "term": "Backpropagation",
    "category": "Training",
    "pronunciation": "/ˌbækprɒpəˈɡeɪʃən/",
    "definition": "An algorithm for training neural networks by calculating gradients of the loss function with respect to weights.",
    "explanation": "Backpropagation uses the chain rule to efficiently compute gradients layer by layer, from output to input. These gradients guide weight updates during training through gradient descent optimization.",
    "examples": ["Training deep neural networks", "Optimizing convolutional layers", "Fine-tuning language models"],
    "relatedTerms": ["gradient-descent", "neural-network", "loss-function"]
  },
  {
    "slug": "overfitting",
    "language": "en",
    "term": "Overfitting",
    "category": "Concept",
    "pronunciation": "/ˌoʊvərˈfɪtɪŋ/",
    "definition": "When a model learns training data too well, including noise, resulting in poor generalization to new data.",
    "explanation": "Overfitting occurs when models are too complex relative to training data size. Solutions include regularization, dropout, early stopping, and data augmentation. Balance between underfitting and overfitting is crucial.",
    "examples": ["A decision tree memorizing all training examples", "Neural network with too many parameters", "Model performing 100% on training but 60% on test data"],
    "relatedTerms": ["regularization", "dropout", "generalization"]
  },
  {
    "slug": "gradient-descent",
    "language": "en",
    "term": "Gradient Descent",
    "category": "Algorithm",
    "pronunciation": "/ˈɡreɪdiənt dɪˈsɛnt/",
    "definition": "An optimization algorithm that iteratively adjusts parameters to minimize a loss function by following the gradient.",
    "explanation": "Gradient descent updates parameters in the direction opposite to the gradient. Variants include batch, mini-batch, and stochastic gradient descent (SGD). Learning rate controls step size.",
    "examples": ["Training neural network weights", "Optimizing linear regression", "Fine-tuning model parameters"],
    "relatedTerms": ["backpropagation", "optimizer", "learning-rate"]
  },
  {
    "slug": "reinforcement-learning",
    "language": "en",
    "term": "Reinforcement Learning",
    "category": "Paradigm",
    "pronunciation": "/ˌriːɪnˈfɔːrsmənt ˈlɜːrnɪŋ/",
    "definition": "A machine learning paradigm where agents learn by interacting with an environment and receiving rewards or penalties.",
    "explanation": "RL agents learn optimal policies through trial and error. Key concepts include states, actions, rewards, and Q-learning. Used in game playing (AlphaGo), robotics, and autonomous systems.",
    "examples": ["AlphaGo defeating world champions", "Robotic control systems", "Autonomous driving decisions"],
    "relatedTerms": ["q-learning", "policy-gradient", "deep-q-network"]
  },
  {
    "slug": "fine-tuning",
    "language": "en",
    "term": "Fine-tuning",
    "category": "Training",
    "pronunciation": "/faɪn ˈtjuːnɪŋ/",
    "definition": "The process of adapting a pre-trained model to a specific task by continuing training on task-specific data.",
    "explanation": "Fine-tuning leverages transfer learning by starting with pre-trained weights and updating them with smaller learning rates on new data. More efficient than training from scratch.",
    "examples": ["Adapting BERT for sentiment analysis", "Fine-tuning GPT for code generation", "Customizing vision models for medical imaging"],
    "relatedTerms": ["transfer-learning", "pre-training", "llm"]
  },
  {
    "slug": "attention-mechanism",
    "language": "en",
    "term": "Attention Mechanism",
    "category": "Technique",
    "pronunciation": "/əˈtɛnʃən ˈmɛkənɪzəm/",
    "definition": "A technique allowing models to focus on specific parts of the input when producing output.",
    "explanation": "Attention assigns importance weights to different input elements. Used in machine translation, image captioning, and transformers. Enables models to handle long sequences effectively.",
    "examples": ["Focusing on relevant words in translation", "Highlighting important image regions", "Multi-head attention in transformers"],
    "relatedTerms": ["self-attention", "transformer", "encoder-decoder"]
  },
  {
    "slug": "diffusion-models",
    "language": "en",
    "term": "Diffusion Models",
    "category": "Architecture",
    "definition": "Generative models that learn to create data by reversing a gradual noising process.",
    "explanation": "Diffusion models add noise to data incrementally, then learn to denoise. State-of-the-art for image generation (Stable Diffusion, DALL-E). Produce high-quality, diverse outputs.",
    "examples": ["Stable Diffusion for text-to-image", "DALL-E 3 for creative imagery", "Imagen for photorealistic generation"],
    "relatedTerms": ["stable-diffusion", "generative-ai", "gan"]
  },
  {
    "slug": "rag",
    "language": "en",
    "term": "RAG (Retrieval-Augmented Generation)",
    "category": "Technique",
    "pronunciation": "/ræɡ/",
    "definition": "A technique that enhances LLM outputs by retrieving relevant information from external knowledge bases.",
    "explanation": "RAG combines retrieval systems with generative models. First retrieves relevant documents, then uses them as context for generation. Reduces hallucinations and enables knowledge updates without retraining.",
    "examples": ["Question answering with document retrieval", "Customer support chatbots", "Research assistants"],
    "relatedTerms": ["llm", "vector-database", "semantic-search"]
  },
  {
    "slug": "transfer-learning",
    "language": "it",
    "term": "Transfer Learning",
    "category": "Addestramento",
    "definition": "Una tecnica dove la conoscenza appresa da un task viene applicata a un task diverso ma correlato, riducendo tempo di training e requisiti dati.",
    "explanation": "Il transfer learning sfrutta modelli pre-addestrati. Approccio comune: usare un modello addestrato su grande dataset (es. ImageNet) come punto di partenza per task specifico.",
    "examples": ["Usare ResNet pre-addestrato per classificazione immagini mediche", "Fine-tuning di BERT per sentiment analysis", "Adattare GPT per generazione codice"],
    "relatedTerms": ["fine-tuning", "pre-training", "neural-network"]
  },
  {
    "slug": "rete-convoluzionale",
    "language": "it",
    "term": "Rete Convoluzionale",
    "category": "Architettura",
    "pronunciation": "/ˈrete konvolutsjoːnale/",
    "definition": "Una rete convoluzionale è un tipo di rete neurale progettata per elaborare dati strutturati in griglie, come immagini, attraverso l'uso di operazioni di convoluzione.",
    "explanation": "# Rete Convoluzionale\n\n## Introduzione\nLe reti convoluzionali (CNN) sono un'architettura di apprendimento profondo ampiamente utilizzata nel campo della visione artificiale. Queste reti sono particolarmente efficaci nell'estrazione di caratteristiche da dati ad alta dimensione come le immagini.\n\n## Come Funziona\nUna rete convoluzionale è composta da vari strati:\n- **Strati di convoluzione**: Applicano filtri per estrarre caratteristiche\n- **Strati di pooling**: Riducono la dimensionalità mantenendo le caratteristiche più significative\n- **Strati completamente connessi**: Producono le classificazioni finali\n\n## Applicazioni\n- Riconoscimento facciale\n- Classificazione delle immagini\n- Guida autonoma",
    "examples": ["Un sistema di riconoscimento facciale che utilizza una CNN per identificare gli individui in tempo reale.", "Un'applicazione di classificazione delle immagini", "Un software di diagnostica medica che utilizza reti convoluzionali"],
    "relatedTerms": ["rete-neurale", "deep-learning", "visione-artificiale"],
    "etymology": "Il termine 'rete convoluzionale' deriva dalla parola 'convoluzione', un'operazione matematica fondamentale utilizzata per l'estrazione delle caratteristiche nelle reti neurali."
  },
  {
    "slug": "self-attention",
    "language": "it",
    "term": "Self Attention",
    "category": "Tecnica",
    "pronunciation": "/sɛlf əˈtɛnʃən/",
    "definition": "Il self attention è un meccanismo di attenzione utilizzato nei modelli di deep learning che consente a una rete neurale di pesare l'importanza di diverse parti di un input rispetto ad altre.",
    "explanation": "# Self Attention\n\nIl self attention è una tecnica fondamentale nei modelli di linguaggio e visione artificiale. Permette ai modelli di considerare le relazioni tra elementi all'interno di una singola sequenza.\n\n## Come Funziona\n1. **Input**: Set di vettori di input\n2. **Calcolo dei Pesi**: Query, key e value\n3. **Attenzione**: Matrice di attenzione con softmax\n4. **Aggregazione**: Output finale contestuale\n\n## Caratteristiche\n- Contesto dinamico\n- Scalabilità\n- Parallelizzazione\n\n## Applicazioni\n- Traduzione automatica\n- NLP\n- Visione artificiale",
    "examples": ["Un modello di traduzione automatica che utilizza self attention", "Un sistema di raccomandazione", "Un modello di generazione di testo"],
    "relatedTerms": ["transformer", "attention-mechanism", "deep-learning"],
    "etymology": "Il termine 'self attention' deriva dall'idea di attenzione applicata a se stessa, in cui ogni parte di un input può influenzare l'interpretazione di altre parti."
  },
  {
    "slug": "machine-learning",
    "language": "it",
    "term": "Machine Learning",
    "category": "Concetto",
    "definition": "Una branca dell'intelligenza artificiale che permette ai computer di apprendere da dati senza essere esplicitamente programmati.",
    "explanation": "Il machine learning usa algoritmi per identificare pattern nei dati e fare predizioni. Include supervised learning (con etichette), unsupervised learning (senza etichette), e reinforcement learning (con ricompense).",
    "examples": ["Classificazione di email spam", "Raccomandazioni di prodotti", "Riconoscimento vocale"],
    "relatedTerms": ["deep-learning", "supervised-learning", "neural-network"]
  },
  {
    "slug": "bert",
    "language": "it",
    "term": "BERT",
    "category": "Modello",
    "definition": "Bidirectional Encoder Representations from Transformers - un modello di linguaggio pre-addestrato bidirezionale.",
    "explanation": "BERT analizza il testo in entrambe le direzioni per comprendere meglio il contesto. Pre-addestrato su enormi corpus di testo, poi fine-tuned per task specifici come Q&A, sentiment analysis, NER.",
    "examples": ["Google Search per comprendere query complesse", "Analisi del sentiment di recensioni", "Estrazione di entità da testi"],
    "relatedTerms": ["transformer", "gpt", "nlp"]
  },
  {
    "slug": "dropout",
    "language": "it",
    "term": "Dropout",
    "category": "Tecnica",
    "definition": "Una tecnica di regolarizzazione che disattiva casualmente neuroni durante il training per prevenire overfitting.",
    "explanation": "Durante ogni iterazione di training, il dropout rimuove temporaneamente una percentuale casuale di neuroni. Questo forza la rete ad apprendere rappresentazioni più robuste e generalizzabili.",
    "examples": ["Dropout con rate 0.5 in layer densi", "Riduzione overfitting in reti profonde", "Miglioramento generalizzazione"],
    "relatedTerms": ["overfitting", "regularization", "neural-network"]
  },
  {
    "slug": "embedding",
    "language": "it",
    "term": "Embedding",
    "category": "Tecnica",
    "definition": "Una rappresentazione vettoriale densa di entità discrete (parole, immagini) in uno spazio continuo.",
    "explanation": "Gli embedding mappano entità in vettori dove elementi simili sono vicini nello spazio. Word2Vec e GloVe creano word embeddings. Essenziali per NLP e sistemi di raccomandazione.",
    "examples": ["Word2Vec per rappresentare parole", "Embedding di prodotti per raccomandazioni", "Sentence embeddings per similarità semantica"],
    "relatedTerms": ["word2vec", "vector-space", "similarity"]
  },
  {
    "slug": "batch-normalization",
    "language": "it",
    "term": "Batch Normalization",
    "category": "Tecnica",
    "definition": "Una tecnica per normalizzare gli input di ogni layer, stabilizzando e accelerando il training.",
    "explanation": "Batch normalization normalizza le attivazioni usando media e varianza del batch. Riduce internal covariate shift, permette learning rates più alti, e agisce come regolarizzatore.",
    "examples": ["Stabilizzare training di reti profonde", "Accelerare convergenza", "Migliorare performance su vision tasks"],
    "relatedTerms": ["layer-normalization", "deep-learning", "training"]
  },
  {
    "slug": "visione-artificiale",
    "language": "it",
    "term": "Visione Artificiale",
    "category": "Campo",
    "definition": "Un campo dell'AI che permette ai computer di interpretare e comprendere informazioni visive dal mondo.",
    "explanation": "La visione artificiale usa deep learning per analizzare immagini e video. Applicazioni includono riconoscimento oggetti, segmentazione, pose estimation, e autonomous driving.",
    "examples": ["Riconoscimento facciale", "Autonomous driving", "Diagnostica medica da immagini"],
    "relatedTerms": ["rete-convoluzionale", "object-detection", "segmentation"]
  },
  {
    "slug": "nlp",
    "language": "it",
    "term": "NLP (Natural Language Processing)",
    "category": "Campo",
    "definition": "Elaborazione del linguaggio naturale - campo dell'AI che permette ai computer di comprendere e generare linguaggio umano.",
    "explanation": "NLP combina linguistica computazionale e machine learning. Task comuni: sentiment analysis, traduzione automatica, question answering, text generation. Transformers hanno rivoluzionato il campo.",
    "examples": ["ChatGPT per conversazioni", "Google Translate", "Assistenti vocali come Alexa"],
    "relatedTerms": ["transformer", "bert", "gpt"]
  },
  {
    "slug": "ai-generativa",
    "language": "it",
    "term": "AI Generativa",
    "category": "Concetto",
    "definition": "Sistemi di intelligenza artificiale capaci di creare nuovi contenuti originali come testo, immagini, audio, video.",
    "explanation": "L'AI generativa usa modelli come GPT (testo), DALL-E (immagini), Stable Diffusion (immagini). Apprende pattern dai dati di training e genera contenuti nuovi ma realistici.",
    "examples": ["ChatGPT per scrittura creativa", "Midjourney per arte digitale", "Suno per generazione musicale"],
    "relatedTerms": ["gpt", "diffusion-models", "gan"]
  },
  {
    "slug": "cnn",
    "language": "en",
    "term": "CNN (Convolutional Neural Network)",
    "category": "Architecture",
    "pronunciation": "/siː ɛn ɛn/",
    "definition": "A deep learning architecture specialized for processing grid-like data such as images, using convolutional layers.",
    "explanation": "CNNs use convolution operations to automatically learn spatial hierarchies of features. Key components include convolutional layers, pooling layers, and fully connected layers. Revolutionized computer vision.",
    "examples": ["ResNet for image classification", "YOLO for object detection", "U-Net for image segmentation"],
    "relatedTerms": ["convolution", "pooling", "computer-vision"]
  },
  {
    "slug": "gan",
    "language": "en",
    "term": "GAN (Generative Adversarial Network)",
    "category": "Architecture",
    "pronunciation": "/ɡæn/",
    "definition": "A framework where two neural networks compete: a generator creates fake data and a discriminator tries to distinguish real from fake.",
    "explanation": "GANs consist of a generator and discriminator trained adversarially. The generator improves at creating realistic data while the discriminator improves at detection. Used for image generation, style transfer, and data augmentation.",
    "examples": ["StyleGAN for face generation", "CycleGAN for image-to-image translation", "Pix2Pix for paired image translation"],
    "relatedTerms": ["generator", "discriminator", "generative-ai"]
  },
  {
    "slug": "activation-function",
    "language": "en",
    "term": "Activation Function",
    "category": "Concept",
    "pronunciation": "/ˌæktɪˈveɪʃən ˈfʌŋkʃən/",
    "definition": "A mathematical function applied to a neuron's output to introduce non-linearity into the network.",
    "explanation": "Activation functions enable neural networks to learn complex patterns. Common functions include ReLU, sigmoid, tanh, and softmax. They determine whether a neuron should be activated based on input.",
    "examples": ["ReLU for hidden layers", "Sigmoid for binary classification", "Softmax for multi-class classification"],
    "relatedTerms": ["relu", "sigmoid", "neural-network"]
  },
  {
    "slug": "loss-function",
    "language": "en",
    "term": "Loss Function",
    "category": "Concept",
    "pronunciation": "/lɒs ˈfʌŋkʃən/",
    "definition": "A function that measures the difference between predicted and actual values, guiding model optimization.",
    "explanation": "Loss functions quantify model error. Common types include Mean Squared Error (MSE) for regression, Cross-Entropy for classification. Optimization minimizes loss through gradient descent.",
    "examples": ["MSE for regression tasks", "Cross-entropy for classification", "Huber loss for robust regression"],
    "relatedTerms": ["gradient-descent", "optimization", "backpropagation"]
  },
  {
    "slug": "regularization",
    "language": "en",
    "term": "Regularization",
    "category": "Technique",
    "pronunciation": "/ˌrɛɡjʊləraɪˈzeɪʃən/",
    "definition": "Techniques to prevent overfitting by adding constraints or penalties to the model during training.",
    "explanation": "Regularization methods include L1 (Lasso), L2 (Ridge), dropout, and early stopping. They reduce model complexity and improve generalization to new data.",
    "examples": ["L2 regularization in linear regression", "Dropout in neural networks", "Early stopping to prevent overtraining"],
    "relatedTerms": ["overfitting", "dropout", "generalization"]
  },
  {
    "slug": "encoder-decoder",
    "language": "en",
    "term": "Encoder-Decoder",
    "category": "Architecture",
    "pronunciation": "/ɪnˈkoʊdər dɪˈkoʊdər/",
    "definition": "An architecture where an encoder processes input into a representation and a decoder generates output from it.",
    "explanation": "Encoder-decoder architectures are used for sequence-to-sequence tasks. The encoder compresses input into a context vector, the decoder generates output. Common in machine translation and summarization.",
    "examples": ["Seq2Seq for translation", "Transformer encoder-decoder", "VAE (Variational Autoencoder)"],
    "relatedTerms": ["transformer", "sequence-to-sequence", "attention-mechanism"]
  },
  {
    "slug": "tokenization",
    "language": "en",
    "term": "Tokenization",
    "category": "Technique",
    "pronunciation": "/ˌtoʊkənaɪˈzeɪʃən/",
    "definition": "The process of breaking text into smaller units (tokens) like words, subwords, or characters for processing.",
    "explanation": "Tokenization is the first step in NLP pipelines. Methods include word-level, character-level, and subword tokenization (BPE, WordPiece). Balances vocabulary size and representation quality.",
    "examples": ["BPE in GPT models", "WordPiece in BERT", "SentencePiece for multilingual models"],
    "relatedTerms": ["nlp", "bert", "gpt"]
  },
  {
    "slug": "zero-shot-learning",
    "language": "en",
    "term": "Zero-Shot Learning",
    "category": "Concept",
    "pronunciation": "/ˈzɪroʊ ʃɒt ˈlɜːrnɪŋ/",
    "definition": "A model's ability to perform tasks it wasn't explicitly trained on, using only task descriptions or examples.",
    "explanation": "Zero-shot learning leverages pre-trained knowledge to handle new tasks without additional training. Large language models excel at this through prompt engineering.",
    "examples": ["GPT-3 translating to languages not in training", "CLIP matching images to unseen concepts", "ChatGPT solving novel reasoning tasks"],
    "relatedTerms": ["few-shot-learning", "prompt-engineering", "llm"]
  },
  {
    "slug": "prompt-engineering",
    "language": "en",
    "term": "Prompt Engineering",
    "category": "Technique",
    "pronunciation": "/prɒmpt ˌɛndʒɪˈnɪərɪŋ/",
    "definition": "The practice of designing effective text prompts to guide large language models toward desired outputs.",
    "explanation": "Prompt engineering involves crafting instructions, examples, and context to optimize LLM performance. Techniques include few-shot prompting, chain-of-thought, and role-playing.",
    "examples": ["Few-shot prompting with examples", "Chain-of-thought for reasoning", "System prompts for chatbot behavior"],
    "relatedTerms": ["llm", "gpt", "zero-shot-learning"]
  },
  {
    "slug": "rete-neurale",
    "language": "it",
    "term": "Rete Neurale",
    "category": "Architettura",
    "definition": "Un modello computazionale ispirato alle reti neurali biologiche, composto da nodi interconnessi (neuroni) che elaborano informazioni.",
    "explanation": "Le reti neurali sono modelli di machine learning ispirati alla struttura del cervello umano. Consistono di layer di nodi interconnessi che processano e trasformano dati di input per produrre predizioni.",
    "examples": ["Classificazione di immagini", "Riconoscimento vocale", "Elaborazione del linguaggio naturale"],
    "relatedTerms": ["deep-learning", "backpropagation", "activation-function"]
  },
  {
    "slug": "ottimizzazione",
    "language": "it",
    "term": "Ottimizzazione",
    "category": "Concetto",
    "definition": "Il processo di regolazione dei parametri del modello per minimizzare la funzione di loss e migliorare le performance.",
    "explanation": "L'ottimizzazione usa algoritmi come gradient descent, Adam, e RMSprop per aggiornare i pesi del modello. L'obiettivo è trovare i parametri ottimali che minimizzano l'errore di predizione.",
    "examples": ["Adam optimizer per training veloce", "SGD con momentum", "Learning rate scheduling"],
    "relatedTerms": ["gradient-descent", "loss-function", "training"]
  },
  {
    "slug": "data-augmentation",
    "language": "it",
    "term": "Data Augmentation",
    "category": "Tecnica",
    "definition": "Tecniche per aumentare artificialmente la dimensione del dataset di training attraverso trasformazioni dei dati esistenti.",
    "explanation": "Il data augmentation crea variazioni dei dati originali (rotazioni, crop, flip per immagini; sinonimi, back-translation per testo) per migliorare la generalizzazione e ridurre overfitting.",
    "examples": ["Rotazione e flip di immagini", "Sinonimi e parafrasamento per testo", "Noise injection per audio"],
    "relatedTerms": ["overfitting", "training", "generalization"]
  },
  {
    "slug": "pre-training",
    "language": "it",
    "term": "Pre-training",
    "category": "Addestramento",
    "definition": "Il processo di addestrare un modello su un grande dataset generico prima di fine-tuning su task specifici.",
    "explanation": "Il pre-training permette ai modelli di apprendere rappresentazioni generali da grandi quantità di dati non etichettati. Fondamentale per transfer learning e modelli foundation come BERT e GPT.",
    "examples": ["BERT pre-trained su Wikipedia", "GPT pre-trained su web text", "ResNet pre-trained su ImageNet"],
    "relatedTerms": ["fine-tuning", "transfer-learning", "foundation-models"]
  },
  {
    "slug": "tensore",
    "language": "it",
    "term": "Tensore",
    "category": "Concetto",
    "definition": "Una struttura dati multi-dimensionale utilizzata per rappresentare dati in deep learning (scalari, vettori, matrici, tensori n-dimensionali).",
    "explanation": "I tensori sono la struttura dati fondamentale in framework come TensorFlow e PyTorch. Un tensore 0D è uno scalare, 1D è un vettore, 2D è una matrice, e così via.",
    "examples": ["Tensore 2D per immagine grayscale", "Tensore 3D per immagine RGB", "Tensore 4D per batch di immagini"],
    "relatedTerms": ["pytorch", "tensorflow", "array"]
  },
  {
    "slug": "cross-entropy",
    "language": "it",
    "term": "Cross-Entropy",
    "category": "Concetto",
    "definition": "Una funzione di loss comunemente usata per problemi di classificazione che misura la differenza tra distribuzioni di probabilità.",
    "explanation": "La cross-entropy quantifica quanto le predizioni del modello divergono dalle etichette reali. Valori più bassi indicano predizioni migliori. Usata con softmax per classificazione multi-classe.",
    "examples": ["Classificazione di immagini", "Sentiment analysis", "Named Entity Recognition"],
    "relatedTerms": ["loss-function", "softmax", "classification"]
  },
  {
    "slug": "lstm",
    "language": "it",
    "term": "LSTM (Long Short-Term Memory)",
    "category": "Architettura",
    "definition": "Un tipo di rete neurale ricorrente capace di apprendere dipendenze a lungo termine in sequenze temporali.",
    "explanation": "LSTM usa celle di memoria e gate (input, forget, output) per gestire il flusso di informazioni. Risolve il problema del vanishing gradient delle RNN standard. Ora spesso sostituita da Transformer.",
    "examples": ["Previsioni serie temporali", "Generazione di testo", "Riconoscimento vocale"],
    "relatedTerms": ["rnn", "gru", "sequence-modeling"]
  },
  {
    "slug": "batch-size",
    "language": "en",
    "term": "Batch Size",
    "category": "Training",
    "pronunciation": "/bætʃ saɪz/",
    "definition": "The number of training examples used in one iteration of model training.",
    "explanation": "Batch size affects training speed, memory usage, and model convergence. Small batches provide noisier gradients, large batches are more stable but memory-intensive. Common values: 32, 64, 128, 256.",
    "examples": ["Batch size 32 for limited GPU memory", "Batch size 256 for faster training", "Mini-batch gradient descent"],
    "relatedTerms": ["gradient-descent", "training", "optimization"]
  },
  {
    "slug": "learning-rate",
    "language": "en",
    "term": "Learning Rate",
    "category": "Training",
    "pronunciation": "/ˈlɜːrnɪŋ reɪt/",
    "definition": "A hyperparameter controlling how much model weights are updated during training.",
    "explanation": "Learning rate determines the step size in gradient descent. Too high causes instability, too low slows training. Common strategies: fixed, decay, cyclical, adaptive (Adam).",
    "examples": ["Learning rate 0.001 for Adam", "Learning rate decay schedule", "Warm-up then decay strategy"],
    "relatedTerms": ["gradient-descent", "optimization", "hyperparameter"]
  },
  {
    "slug": "epoch",
    "language": "en",
    "term": "Epoch",
    "category": "Training",
    "pronunciation": "/ˈɛpɒk/",
    "definition": "One complete pass through the entire training dataset during model training.",
    "explanation": "Multiple epochs allow the model to learn patterns iteratively. Too few epochs lead to underfitting, too many to overfitting. Monitored with validation loss for early stopping.",
    "examples": ["Training for 100 epochs", "Early stopping at epoch 45", "Learning curves over epochs"],
    "relatedTerms": ["training", "overfitting", "early-stopping"]
  },
  {
    "slug": "feature-extraction",
    "language": "en",
    "term": "Feature Extraction",
    "category": "Technique",
    "pronunciation": "/ˈfiːtʃər ɪkˈstrækʃən/",
    "definition": "The process of transforming raw data into numerical features that machine learning models can process.",
    "explanation": "Feature extraction identifies relevant information from data. In deep learning, this happens automatically through learned representations. Traditional ML requires manual feature engineering.",
    "examples": ["Edge detection in images", "TF-IDF for text", "Convolutional features in CNNs"],
    "relatedTerms": ["feature-engineering", "representation-learning", "cnn"]
  },
  {
    "slug": "computer-vision",
    "language": "en",
    "term": "Computer Vision",
    "category": "Field",
    "pronunciation": "/kəmˈpjuːtər ˈvɪʒən/",
    "definition": "A field of AI enabling computers to derive meaningful information from visual inputs like images and videos.",
    "explanation": "Computer vision tasks include image classification, object detection, segmentation, and pose estimation. Powered by CNNs and transformer-based models like Vision Transformers (ViT).",
    "examples": ["Facial recognition systems", "Autonomous vehicle perception", "Medical image analysis"],
    "relatedTerms": ["cnn", "object-detection", "segmentation"]
  },
  {
    "slug": "object-detection",
    "language": "en",
    "term": "Object Detection",
    "category": "Task",
    "pronunciation": "/ˈɒbdʒɪkt dɪˈtɛkʃən/",
    "definition": "A computer vision task that identifies and localizes objects within an image using bounding boxes.",
    "explanation": "Object detection combines classification and localization. Popular architectures include YOLO (real-time), Faster R-CNN (accurate), and DETR (transformer-based).",
    "examples": ["YOLO for real-time detection", "Faster R-CNN for high accuracy", "Pedestrian detection in autonomous driving"],
    "relatedTerms": ["computer-vision", "cnn", "bounding-box"]
  },
  {
    "slug": "semantic-segmentation",
    "language": "en",
    "term": "Semantic Segmentation",
    "category": "Task",
    "pronunciation": "/sɪˈmæntɪk ˌsɛɡmɛnˈteɪʃən/",
    "definition": "A computer vision task that assigns a class label to every pixel in an image.",
    "explanation": "Semantic segmentation creates dense predictions, useful for understanding scene layout. Common architectures include U-Net, DeepLab, and Mask R-CNN for instance segmentation.",
    "examples": ["Medical image segmentation", "Autonomous driving scene understanding", "Satellite image analysis"],
    "relatedTerms": ["computer-vision", "u-net", "instance-segmentation"]
  },
  {
    "slug": "multimodal",
    "language": "en",
    "term": "Multimodal AI",
    "category": "Concept",
    "pronunciation": "/ˌmʌltɪˈmoʊdəl/",
    "definition": "AI systems that can process and relate information from multiple modalities like text, images, audio, and video.",
    "explanation": "Multimodal models learn joint representations across modalities. Examples include CLIP (vision-language), Flamingo (visual question answering), and GPT-4V (vision understanding).",
    "examples": ["CLIP matching images to text", "GPT-4V describing images", "Image captioning systems"],
    "relatedTerms": ["clip", "vision-language", "cross-modal"]
  },
  {
    "slug": "few-shot-learning",
    "language": "en",
    "term": "Few-Shot Learning",
    "category": "Concept",
    "pronunciation": "/fjuː ʃɒt ˈlɜːrnɪŋ/",
    "definition": "A model's ability to learn from a small number of examples, typically 1-10 examples per class.",
    "explanation": "Few-shot learning leverages pre-trained knowledge and meta-learning. LLMs demonstrate this through in-context learning with examples in the prompt.",
    "examples": ["GPT-3 with 5 examples in prompt", "One-shot image classification", "Meta-learning algorithms"],
    "relatedTerms": ["zero-shot-learning", "meta-learning", "prompt-engineering"]
  },
  {
    "slug": "foundation-model",
    "language": "en",
    "term": "Foundation Model",
    "category": "Concept",
    "pronunciation": "/faʊnˈdeɪʃən ˈmɒdəl/",
    "definition": "Large-scale models trained on broad data that can be adapted to a wide range of downstream tasks.",
    "explanation": "Foundation models like GPT, BERT, and CLIP are pre-trained on massive datasets and fine-tuned for specific applications. They represent a paradigm shift in AI development.",
    "examples": ["GPT-4 as a foundation for many applications", "BERT for NLP tasks", "SAM for image segmentation"],
    "relatedTerms": ["llm", "pre-training", "transfer-learning"]
  },
  {
    "slug": "federated-learning",
    "language": "it",
    "term": "Federated Learning",
    "category": "Paradigma",
    "definition": "Un approccio di machine learning che addestra modelli su dati distribuiti senza centralizzarli, preservando la privacy.",
    "explanation": "Il federated learning permette di addestrare modelli su dispositivi edge (smartphone, ospedali) senza trasferire dati sensibili. Il modello globale aggrega gli aggiornamenti locali.",
    "examples": ["Keyboard prediction su smartphone", "Modelli medici tra ospedali", "Google Gboard"],
    "relatedTerms": ["privacy", "distributed-learning", "edge-computing"]
  },
  {
    "slug": "generazione-linguaggio",
    "language": "it",
    "term": "Generazione del Linguaggio",
    "category": "Task",
    "definition": "Il task di produrre testo coerente e contestualmente appropriato da parte di un modello AI.",
    "explanation": "La generazione del linguaggio usa modelli autoregressivi come GPT che predicono token successivi. Tecniche includono beam search, temperature sampling, e nucleus sampling.",
    "examples": ["ChatGPT che genera risposte", "Completamento automatico", "Summarization"],
    "relatedTerms": ["gpt", "llm", "autoregressive"]
  },
  {
    "slug": "knowledge-distillation",
    "language": "it",
    "term": "Knowledge Distillation",
    "category": "Tecnica",
    "definition": "Tecnica per trasferire conoscenza da un modello grande (teacher) a uno più piccolo (student) mantenendo performance simili.",
    "explanation": "Il knowledge distillation crea modelli compatti che possono girare su dispositivi con risorse limitate. Lo student impara dalle soft predictions del teacher.",
    "examples": ["DistilBERT da BERT", "Modelli mobile da modelli cloud", "Edge deployment"],
    "relatedTerms": ["model-compression", "transfer-learning", "edge-ai"]
  },
  {
    "slug": "attention-heads",
    "language": "it",
    "term": "Multi-Head Attention",
    "category": "Tecnica",
    "definition": "Meccanismo che permette al modello di focalizzarsi su diverse parti dell'input simultaneamente attraverso molteplici teste di attenzione.",
    "explanation": "Il multi-head attention usa più meccanismi di attenzione in parallelo, ognuno che impara pattern diversi. Fondamentale nei Transformer. Tipicamente 8-16 heads.",
    "examples": ["BERT con 12 attention heads", "GPT con 96 heads (large)", "Transformer encoder-decoder"],
    "relatedTerms": ["transformer", "self-attention", "bert"]
  },
  {
    "slug": "residual-connection",
    "language": "it",
    "term": "Residual Connection",
    "category": "Tecnica",
    "definition": "Una connessione skip che permette al gradiente di fluire direttamente attraverso la rete, facilitando il training di reti profonde.",
    "explanation": "Le residual connections (o skip connections) sommano l'input di un layer al suo output. Risolvono il problema del vanishing gradient e permettono reti molto profonde (ResNet).",
    "examples": ["ResNet con 152 layers", "Transformer con residual connections", "U-Net skip connections"],
    "relatedTerms": ["resnet", "vanishing-gradient", "deep-learning"]
  },
  {
    "slug": "layer-normalization",
    "language": "it",
    "term": "Layer Normalization",
    "category": "Tecnica",
    "definition": "Tecnica di normalizzazione che opera su tutti i neuroni in un layer, usata principalmente nei Transformer.",
    "explanation": "Layer normalization normalizza le attivazioni attraverso le features invece che attraverso il batch. Più stabile per sequenze di lunghezza variabile. Usata in GPT e BERT.",
    "examples": ["Layer norm nei Transformer", "GPT pre-norm vs post-norm", "Stabilizzazione training"],
    "relatedTerms": ["batch-normalization", "transformer", "normalization"]
  },
  {
    "slug": "gradient-clipping",
    "language": "it",
    "term": "Gradient Clipping",
    "category": "Tecnica",
    "definition": "Tecnica che limita la magnitudine dei gradienti durante il training per prevenire exploding gradients.",
    "explanation": "Il gradient clipping tronca i gradienti a un valore massimo. Essenziale per addestrare RNN e LSTM. Previene aggiornamenti troppo grandi che destabilizzano il training.",
    "examples": ["Clipping a norma 1.0", "Training LSTM stabili", "Prevenzione gradient explosion"],
    "relatedTerms": ["lstm", "gradient-descent", "training"]
  },
  {
    "slug": "autoregressive",
    "language": "it",
    "term": "Autoregressive Model",
    "category": "Concetto",
    "definition": "Modelli che generano output sequenzialmente, dove ogni elemento dipende da quelli precedenti.",
    "explanation": "I modelli autoregressivi come GPT predicono un token alla volta condizionato sui token precedenti. Usano causal masking per prevenire di vedere il futuro durante il training.",
    "examples": ["GPT per generazione testo", "PixelCNN per immagini", "WaveNet per audio"],
    "relatedTerms": ["gpt", "causal-masking", "generation"]
  },
  {
    "slug": "transformer-it",
    "language": "it",
    "term": "Transformer",
    "category": "Architettura",
    "pronunciation": "/trænsˈfɔːrmər/",
    "definition": "Un'architettura di rete neurale basata interamente su meccanismi di attention, senza layer ricorrenti o convoluzionali.",
    "explanation": "I Transformer hanno rivoluzionato l'NLP processando intere sequenze in parallelo usando self-attention. Componenti chiave includono multi-head attention, positional encoding, e feed-forward networks. GPT e BERT si basano su questa architettura.",
    "examples": ["GPT (Generative Pre-trained Transformer)", "BERT (Bidirectional Encoder Representations from Transformers)", "T5 (Text-to-Text Transfer Transformer)"],
    "relatedTerms": ["self-attention", "bert", "gpt"]
  },
  {
    "slug": "gpt-it",
    "language": "it",
    "term": "GPT (Generative Pre-trained Transformer)",
    "category": "Modello",
    "pronunciation": "/dʒiː piː tiː/",
    "definition": "Una famiglia di large language model sviluppati da OpenAI che usano l'architettura transformer per la generazione di testo.",
    "explanation": "I modelli GPT sono addestrati su enormi corpus testuali usando unsupervised learning, poi fine-tuned per task specifici. Eccellono in generazione testo, traduzione, summarization e question-answering. GPT-4 è uno dei modelli più avanzati.",
    "examples": ["ChatGPT per AI conversazionale", "Generazione codice con GitHub Copilot", "Assistenza scrittura creativa"],
    "relatedTerms": ["transformer", "llm", "bert"]
  },
  {
    "slug": "llm-it",
    "language": "it",
    "term": "LLM (Large Language Model)",
    "category": "Modello",
    "pronunciation": "/ɛl ɛl ɛm/",
    "definition": "Una rete neurale con miliardi di parametri addestrata su enormi dataset testuali per comprendere e generare linguaggio umano.",
    "explanation": "Gli LLM come GPT-4, Claude e LLaMA sono addestrati su testi diversificati da internet. Dimostrano abilità emergenti come ragionamento, few-shot learning e generalizzazione dei task. La scala è fondamentale - modelli più grandi mostrano performance migliori.",
    "examples": ["GPT-4 con 1.76 trilioni di parametri", "LLaMA 2 per applicazioni open-source", "Claude per comprensione long-context"],
    "relatedTerms": ["gpt", "transformer", "fine-tuning"]
  },
  {
    "slug": "overfitting-it",
    "language": "it",
    "term": "Overfitting",
    "category": "Concetto",
    "pronunciation": "/ˌoʊvərˈfɪtɪŋ/",
    "definition": "Quando un modello apprende i dati di training troppo bene, incluso il rumore, risultando in scarsa generalizzazione su nuovi dati.",
    "explanation": "L'overfitting si verifica quando i modelli sono troppo complessi rispetto alla dimensione dei dati di training. Le soluzioni includono regolarizzazione, dropout, early stopping e data augmentation. Cruciale trovare il bilancio tra underfitting e overfitting.",
    "examples": ["Decision tree che memorizza tutti gli esempi", "Rete neurale con troppi parametri", "Modello con 100% su training ma 60% su test"],
    "relatedTerms": ["regularization", "dropout", "generalization"]
  },
  {
    "slug": "cnn-it",
    "language": "it",
    "term": "CNN (Convolutional Neural Network)",
    "category": "Architettura",
    "pronunciation": "/siː ɛn ɛn/",
    "definition": "Un'architettura di deep learning specializzata per processare dati a griglia come immagini, usando layer convoluzionali.",
    "explanation": "Le CNN usano operazioni di convoluzione per apprendere automaticamente gerarchie spaziali di features. Componenti chiave includono layer convoluzionali, pooling e fully connected. Hanno rivoluzionato la computer vision.",
    "examples": ["ResNet per classificazione immagini", "YOLO per object detection", "U-Net per segmentazione"],
    "relatedTerms": ["convolution", "pooling", "computer-vision"]
  },
  {
    "slug": "gan-it",
    "language": "it",
    "term": "GAN (Generative Adversarial Network)",
    "category": "Architettura",
    "pronunciation": "/ɡæn/",
    "definition": "Un framework dove due reti neurali competono: un generator crea dati falsi e un discriminator cerca di distinguere vero da falso.",
    "explanation": "Le GAN consistono di generator e discriminator addestrati in modo adversarial. Il generator migliora nel creare dati realistici mentre il discriminator migliora nel rilevarli. Usate per generazione immagini, style transfer e data augmentation.",
    "examples": ["StyleGAN per generazione volti", "CycleGAN per traduzione immagine-immagine", "Pix2Pix per traduzioni paired"],
    "relatedTerms": ["generator", "discriminator", "generative-ai"]
  },
  {
    "slug": "rag-it",
    "language": "it",
    "term": "RAG (Retrieval-Augmented Generation)",
    "category": "Tecnica",
    "pronunciation": "/ræɡ/",
    "definition": "Una tecnica che migliora gli output degli LLM recuperando informazioni rilevanti da knowledge base esterne.",
    "explanation": "RAG combina sistemi di retrieval con modelli generativi. Prima recupera documenti rilevanti, poi li usa come contesto per la generazione. Riduce le allucinazioni e permette aggiornamenti della conoscenza senza retraining.",
    "examples": ["Question answering con retrieval documenti", "Chatbot supporto clienti", "Assistenti di ricerca"],
    "relatedTerms": ["llm", "vector-database", "semantic-search"]
  },
  {
    "slug": "reinforcement-learning-it",
    "language": "it",
    "term": "Reinforcement Learning",
    "category": "Paradigma",
    "pronunciation": "/ˌriːɪnˈfɔːrsmənt ˈlɜːrnɪŋ/",
    "definition": "Un paradigma di machine learning dove gli agenti apprendono interagendo con un ambiente e ricevendo ricompense o penalità.",
    "explanation": "Gli agenti RL apprendono policy ottimali attraverso trial and error. Concetti chiave includono stati, azioni, ricompense e Q-learning. Usato in game playing (AlphaGo), robotica e sistemi autonomi.",
    "examples": ["AlphaGo che batte campioni mondiali", "Sistemi di controllo robotico", "Decisioni guida autonoma"],
    "relatedTerms": ["q-learning", "policy-gradient", "deep-q-network"]
  },
  {
    "slug": "tokenization-it",
    "language": "it",
    "term": "Tokenization",
    "category": "Tecnica",
    "pronunciation": "/ˌtoʊkənaɪˈzeɪʃən/",
    "definition": "Il processo di suddividere il testo in unità più piccole (token) come parole, subword o caratteri per l'elaborazione.",
    "explanation": "La tokenization è il primo step nelle pipeline NLP. I metodi includono word-level, character-level e subword tokenization (BPE, WordPiece). Bilancia dimensione vocabolario e qualità rappresentazione.",
    "examples": ["BPE nei modelli GPT", "WordPiece in BERT", "SentencePiece per modelli multilingua"],
    "relatedTerms": ["nlp", "bert", "gpt"]
  },
  {
    "slug": "prompt-engineering-it",
    "language": "it",
    "term": "Prompt Engineering",
    "category": "Tecnica",
    "pronunciation": "/prɒmpt ˌɛndʒɪˈnɪərɪŋ/",
    "definition": "La pratica di progettare prompt testuali efficaci per guidare i large language model verso output desiderati.",
    "explanation": "Il prompt engineering coinvolge la creazione di istruzioni, esempi e contesto per ottimizzare le performance degli LLM. Tecniche includono few-shot prompting, chain-of-thought e role-playing.",
    "examples": ["Few-shot prompting con esempi", "Chain-of-thought per reasoning", "System prompt per comportamento chatbot"],
    "relatedTerms": ["llm", "gpt", "zero-shot-learning"]
  },
  {
    "slug": "transfer-learning-en",
    "language": "en",
    "term": "Transfer Learning",
    "category": "Training",
    "definition": "A technique where knowledge learned from one task is applied to a different but related task, reducing training time and data requirements.",
    "explanation": "Transfer learning leverages pre-trained models. Common approach: use a model trained on large dataset (e.g., ImageNet) as starting point for specific task.",
    "examples": ["Using pre-trained ResNet for medical image classification", "Fine-tuning BERT for sentiment analysis", "Adapting GPT for code generation"],
    "relatedTerms": ["fine-tuning", "pre-training", "neural-network"]
  },
  {
    "slug": "self-attention-en",
    "language": "en",
    "term": "Self-Attention",
    "category": "Technique",
    "pronunciation": "/sɛlf əˈtɛnʃən/",
    "definition": "An attention mechanism used in deep learning models that allows a neural network to weigh the importance of different parts of an input relative to each other.",
    "explanation": "Self-attention is a fundamental technique in language and vision models. It allows models to consider relationships between elements within a single sequence, greatly improving comprehension and generation capabilities.",
    "examples": ["Machine translation models using self-attention", "Recommendation systems", "Text generation models"],
    "relatedTerms": ["transformer", "attention-mechanism", "deep-learning"]
  },
  {
    "slug": "dropout-en",
    "language": "en",
    "term": "Dropout",
    "category": "Technique",
    "definition": "A regularization technique that randomly deactivates neurons during training to prevent overfitting.",
    "explanation": "During each training iteration, dropout temporarily removes a random percentage of neurons. This forces the network to learn more robust and generalizable representations.",
    "examples": ["Dropout with rate 0.5 in dense layers", "Reducing overfitting in deep networks", "Improving generalization"],
    "relatedTerms": ["overfitting", "regularization", "neural-network"]
  },
  {
    "slug": "embedding-en",
    "language": "en",
    "term": "Embedding",
    "category": "Technique",
    "definition": "A dense vector representation of discrete entities (words, images) in a continuous space.",
    "explanation": "Embeddings map entities to vectors where similar elements are close in space. Word2Vec and GloVe create word embeddings. Essential for NLP and recommendation systems.",
    "examples": ["Word2Vec for word representation", "Product embeddings for recommendations", "Sentence embeddings for semantic similarity"],
    "relatedTerms": ["word2vec", "vector-space", "similarity"]
  },
  {
    "slug": "optimization-en",
    "language": "en",
    "term": "Optimization",
    "category": "Concept",
    "definition": "The process of adjusting model parameters to minimize the loss function and improve performance.",
    "explanation": "Optimization uses algorithms like gradient descent, Adam, and RMSprop to update model weights. The goal is to find optimal parameters that minimize prediction error.",
    "examples": ["Adam optimizer for fast training", "SGD with momentum", "Learning rate scheduling"],
    "relatedTerms": ["gradient-descent", "loss-function", "training"]
  },
  {
    "slug": "multimodal-it",
    "language": "it",
    "term": "AI Multimodale",
    "category": "Concetto",
    "pronunciation": "/ˌmʌltɪˈmoʊdəl/",
    "definition": "Sistemi AI che possono processare e correlare informazioni da modalità multiple come testo, immagini, audio e video.",
    "explanation": "I modelli multimodali apprendono rappresentazioni congiunte tra modalità. Esempi includono CLIP (visione-linguaggio), Flamingo (visual question answering) e GPT-4V (comprensione visione).",
    "examples": ["CLIP che matcha immagini a testo", "GPT-4V che descrive immagini", "Sistemi di image captioning"],
    "relatedTerms": ["clip", "vision-language", "cross-modal"]
  },
  {
    "slug": "relu",
    "language": "en",
    "term": "ReLU (Rectified Linear Unit)",
    "category": "Concept",
    "pronunciation": "/ˈriːluː/",
    "definition": "An activation function that outputs the input if positive, otherwise zero: f(x) = max(0, x).",
    "explanation": "ReLU is the most popular activation function in deep learning due to its simplicity and effectiveness. It helps mitigate vanishing gradient problems and enables faster training compared to sigmoid or tanh.",
    "examples": ["Hidden layers in CNNs", "Deep neural networks", "Most modern architectures"],
    "relatedTerms": ["activation-function", "leaky-relu", "neural-network"]
  },
  {
    "slug": "relu-it",
    "language": "it",
    "term": "ReLU (Rectified Linear Unit)",
    "category": "Concetto",
    "pronunciation": "/ˈriːluː/",
    "definition": "Una funzione di attivazione che restituisce l'input se positivo, altrimenti zero: f(x) = max(0, x).",
    "explanation": "ReLU è la funzione di attivazione più popolare nel deep learning per la sua semplicità ed efficacia. Aiuta a mitigare i problemi di vanishing gradient e permette training più veloce rispetto a sigmoid o tanh.",
    "examples": ["Layer nascosti in CNN", "Reti neurali profonde", "Maggior parte architetture moderne"],
    "relatedTerms": ["activation-function", "leaky-relu", "neural-network"]
  },
  {
    "slug": "sigmoid",
    "language": "en",
    "term": "Sigmoid Function",
    "category": "Concept",
    "pronunciation": "/ˈsɪɡmɔɪd/",
    "definition": "An activation function that maps inputs to values between 0 and 1: f(x) = 1/(1 + e^(-x)).",
    "explanation": "Sigmoid is used for binary classification output layers and was historically popular for hidden layers. However, it suffers from vanishing gradient problems in deep networks.",
    "examples": ["Binary classification output", "Logistic regression", "Gate mechanisms in LSTM"],
    "relatedTerms": ["activation-function", "logistic-regression", "binary-classification"]
  },
  {
    "slug": "sigmoid-it",
    "language": "it",
    "term": "Funzione Sigmoide",
    "category": "Concetto",
    "pronunciation": "/ˈsɪɡmɔɪd/",
    "definition": "Una funzione di attivazione che mappa input a valori tra 0 e 1: f(x) = 1/(1 + e^(-x)).",
    "explanation": "La sigmoide è usata per output layer di classificazione binaria ed era storicamente popolare per hidden layers. Tuttavia, soffre di problemi di vanishing gradient in reti profonde.",
    "examples": ["Output classificazione binaria", "Regressione logistica", "Meccanismi gate in LSTM"],
    "relatedTerms": ["activation-function", "logistic-regression", "binary-classification"]
  },
  {
    "slug": "softmax",
    "language": "en",
    "term": "Softmax Function",
    "category": "Concept",
    "pronunciation": "/ˈsɒftmæks/",
    "definition": "An activation function that converts a vector of values into a probability distribution summing to 1.",
    "explanation": "Softmax is used in multi-class classification output layers. It exponentiates each value and normalizes by the sum, producing interpretable probabilities for each class.",
    "examples": ["Multi-class classification", "Language model next-token prediction", "Image classification output"],
    "relatedTerms": ["activation-function", "cross-entropy", "classification"]
  },
  {
    "slug": "softmax-it",
    "language": "it",
    "term": "Funzione Softmax",
    "category": "Concetto",
    "pronunciation": "/ˈsɒftmæks/",
    "definition": "Una funzione di attivazione che converte un vettore di valori in una distribuzione di probabilità che somma a 1.",
    "explanation": "Softmax è usata negli output layer di classificazione multi-classe. Esponenzia ogni valore e normalizza per la somma, producendo probabilità interpretabili per ogni classe.",
    "examples": ["Classificazione multi-classe", "Predizione next-token in language model", "Output classificazione immagini"],
    "relatedTerms": ["activation-function", "cross-entropy", "classification"]
  },
  {
    "slug": "adam-optimizer",
    "language": "en",
    "term": "Adam Optimizer",
    "category": "Algorithm",
    "pronunciation": "/ˈædəm/",
    "definition": "An adaptive learning rate optimization algorithm combining momentum and RMSprop.",
    "explanation": "Adam (Adaptive Moment Estimation) computes adaptive learning rates for each parameter using first and second moment estimates. It's the default optimizer for many deep learning applications due to its robustness.",
    "examples": ["Training transformers", "Deep neural networks", "Default optimizer in many frameworks"],
    "relatedTerms": ["optimization", "gradient-descent", "learning-rate"]
  },
  {
    "slug": "adam-optimizer-it",
    "language": "it",
    "term": "Ottimizzatore Adam",
    "category": "Algoritmo",
    "pronunciation": "/ˈædəm/",
    "definition": "Un algoritmo di ottimizzazione con learning rate adattivo che combina momentum e RMSprop.",
    "explanation": "Adam (Adaptive Moment Estimation) calcola learning rate adattivi per ogni parametro usando stime del primo e secondo momento. È l'ottimizzatore di default per molte applicazioni deep learning per la sua robustezza.",
    "examples": ["Training transformer", "Reti neurali profonde", "Ottimizzatore di default in molti framework"],
    "relatedTerms": ["optimization", "gradient-descent", "learning-rate"]
  },
  {
    "slug": "resnet",
    "language": "en",
    "term": "ResNet (Residual Network)",
    "category": "Architecture",
    "pronunciation": "/ˈrɛznɛt/",
    "definition": "A CNN architecture that uses residual connections (skip connections) to enable training of very deep networks.",
    "explanation": "ResNet introduced skip connections that allow gradients to flow directly through the network, solving the degradation problem in very deep networks. Variants include ResNet-50, ResNet-101, and ResNet-152.",
    "examples": ["Image classification", "Feature extraction backbone", "Transfer learning base"],
    "relatedTerms": ["cnn", "residual-connection", "computer-vision"]
  },
  {
    "slug": "resnet-it",
    "language": "it",
    "term": "ResNet (Residual Network)",
    "category": "Architettura",
    "pronunciation": "/ˈrɛznɛt/",
    "definition": "Un'architettura CNN che usa connessioni residuali (skip connections) per permettere il training di reti molto profonde.",
    "explanation": "ResNet ha introdotto skip connections che permettono ai gradienti di fluire direttamente attraverso la rete, risolvendo il problema di degradazione in reti molto profonde. Varianti includono ResNet-50, ResNet-101 e ResNet-152.",
    "examples": ["Classificazione immagini", "Backbone per feature extraction", "Base per transfer learning"],
    "relatedTerms": ["cnn", "residual-connection", "computer-vision"]
  },
  {
    "slug": "yolo",
    "language": "en",
    "term": "YOLO (You Only Look Once)",
    "category": "Architecture",
    "pronunciation": "/ˈjoʊloʊ/",
    "definition": "A real-time object detection architecture that frames detection as a regression problem.",
    "explanation": "YOLO divides images into a grid and predicts bounding boxes and class probabilities simultaneously. Known for exceptional speed, making it suitable for real-time applications.",
    "examples": ["Real-time video object detection", "Autonomous driving", "Surveillance systems"],
    "relatedTerms": ["object-detection", "computer-vision", "real-time"]
  },
  {
    "slug": "yolo-it",
    "language": "it",
    "term": "YOLO (You Only Look Once)",
    "category": "Architettura",
    "pronunciation": "/ˈjoʊloʊ/",
    "definition": "Un'architettura di object detection real-time che inquadra il detection come problema di regressione.",
    "explanation": "YOLO divide le immagini in una griglia e predice bounding box e probabilità di classe simultaneamente. Conosciuto per velocità eccezionale, rendendolo adatto per applicazioni real-time.",
    "examples": ["Object detection video real-time", "Guida autonoma", "Sistemi di sorveglianza"],
    "relatedTerms": ["object-detection", "computer-vision", "real-time"]
  },
  {
    "slug": "u-net",
    "language": "en",
    "term": "U-Net",
    "category": "Architecture",
    "pronunciation": "/juː nɛt/",
    "definition": "A CNN architecture designed for biomedical image segmentation, featuring an encoder-decoder structure with skip connections.",
    "explanation": "U-Net has a U-shaped architecture with contracting path (encoder) and expansive path (decoder). Skip connections preserve spatial information, making it excellent for segmentation tasks.",
    "examples": ["Medical image segmentation", "Cell detection", "Satellite image analysis"],
    "relatedTerms": ["semantic-segmentation", "encoder-decoder", "medical-imaging"]
  },
  {
    "slug": "u-net-it",
    "language": "it",
    "term": "U-Net",
    "category": "Architettura",
    "pronunciation": "/juː nɛt/",
    "definition": "Un'architettura CNN progettata per segmentazione di immagini biomediche, con struttura encoder-decoder e skip connections.",
    "explanation": "U-Net ha un'architettura a forma di U con path contrattivo (encoder) e path espansivo (decoder). Le skip connections preservano informazioni spaziali, rendendola eccellente per task di segmentazione.",
    "examples": ["Segmentazione immagini mediche", "Rilevamento cellule", "Analisi immagini satellitari"],
    "relatedTerms": ["semantic-segmentation", "encoder-decoder", "medical-imaging"]
  },
  {
    "slug": "vae",
    "language": "en",
    "term": "VAE (Variational Autoencoder)",
    "category": "Architecture",
    "pronunciation": "/viː eɪ iː/",
    "definition": "A generative model that learns a probabilistic latent space representation of data.",
    "explanation": "VAEs encode data into a distribution in latent space (typically Gaussian) and decode samples back to data space. They enable controllable generation and interpolation in latent space.",
    "examples": ["Image generation", "Anomaly detection", "Data compression"],
    "relatedTerms": ["autoencoder", "generative-ai", "latent-space"]
  },
  {
    "slug": "vae-it",
    "language": "it",
    "term": "VAE (Variational Autoencoder)",
    "category": "Architettura",
    "pronunciation": "/viː eɪ iː/",
    "definition": "Un modello generativo che apprende una rappresentazione probabilistica nello spazio latente dei dati.",
    "explanation": "I VAE codificano dati in una distribuzione nello spazio latente (tipicamente Gaussiana) e decodificano campioni tornando allo spazio dati. Permettono generazione controllabile e interpolazione nello spazio latente.",
    "examples": ["Generazione immagini", "Rilevamento anomalie", "Compressione dati"],
    "relatedTerms": ["autoencoder", "generative-ai", "latent-space"]
  },
  {
    "slug": "autoencoder",
    "language": "en",
    "term": "Autoencoder",
    "category": "Architecture",
    "pronunciation": "/ˈɔːtoʊɪnˌkoʊdər/",
    "definition": "A neural network trained to reconstruct its input, learning compressed representations in the process.",
    "explanation": "Autoencoders consist of an encoder that compresses input to a latent representation and a decoder that reconstructs the original input. Used for dimensionality reduction, denoising, and feature learning.",
    "examples": ["Dimensionality reduction", "Image denoising", "Anomaly detection"],
    "relatedTerms": ["vae", "encoder-decoder", "compression"]
  },
  {
    "slug": "autoencoder-it",
    "language": "it",
    "term": "Autoencoder",
    "category": "Architettura",
    "pronunciation": "/ˈɔːtoʊɪnˌkoʊdər/",
    "definition": "Una rete neurale addestrata a ricostruire il suo input, apprendendo rappresentazioni compresse nel processo.",
    "explanation": "Gli autoencoder consistono di un encoder che comprime l'input a una rappresentazione latente e un decoder che ricostruisce l'input originale. Usati per riduzione dimensionalità, denoising e feature learning.",
    "examples": ["Riduzione dimensionalità", "Denoising immagini", "Rilevamento anomalie"],
    "relatedTerms": ["vae", "encoder-decoder", "compression"]
  },
  {
    "slug": "hyperparameter",
    "language": "en",
    "term": "Hyperparameter",
    "category": "Concept",
    "pronunciation": "/ˌhaɪpərpəˈræmɪtər/",
    "definition": "A configuration variable that is set before training and controls the learning process.",
    "explanation": "Hyperparameters are not learned from data but set by the practitioner. Examples include learning rate, batch size, number of layers, and dropout rate. Tuning them is crucial for optimal performance.",
    "examples": ["Learning rate = 0.001", "Batch size = 32", "Number of hidden layers = 5"],
    "relatedTerms": ["learning-rate", "batch-size", "hyperparameter-tuning"]
  },
  {
    "slug": "hyperparameter-it",
    "language": "it",
    "term": "Iperparametro",
    "category": "Concetto",
    "pronunciation": "/ˌhaɪpərpəˈræmɪtər/",
    "definition": "Una variabile di configurazione impostata prima del training che controlla il processo di apprendimento.",
    "explanation": "Gli iperparametri non sono appresi dai dati ma impostati dal praticante. Esempi includono learning rate, batch size, numero di layer e dropout rate. Il loro tuning è cruciale per performance ottimali.",
    "examples": ["Learning rate = 0.001", "Batch size = 32", "Numero hidden layers = 5"],
    "relatedTerms": ["learning-rate", "batch-size", "hyperparameter-tuning"]
  },
  {
    "slug": "validation-set",
    "language": "en",
    "term": "Validation Set",
    "category": "Concept",
    "pronunciation": "/ˌvælɪˈdeɪʃən sɛt/",
    "definition": "A portion of data held out during training to tune hyperparameters and prevent overfitting.",
    "explanation": "The validation set is used to evaluate model performance during training and make decisions about hyperparameters, architecture, or early stopping. It's distinct from the test set which is only used for final evaluation.",
    "examples": ["80% train, 10% validation, 10% test split", "Hyperparameter tuning", "Early stopping criterion"],
    "relatedTerms": ["train-test-split", "overfitting", "cross-validation"]
  },
  {
    "slug": "validation-set-it",
    "language": "it",
    "term": "Validation Set",
    "category": "Concetto",
    "pronunciation": "/ˌvælɪˈdeɪʃən sɛt/",
    "definition": "Una porzione di dati tenuta da parte durante il training per tuning iperparametri e prevenzione overfitting.",
    "explanation": "Il validation set è usato per valutare le performance del modello durante il training e prendere decisioni su iperparametri, architettura o early stopping. È distinto dal test set che è usato solo per valutazione finale.",
    "examples": ["Split 80% train, 10% validation, 10% test", "Tuning iperparametri", "Criterio early stopping"],
    "relatedTerms": ["train-test-split", "overfitting", "cross-validation"]
  },
  {
    "slug": "confusion-matrix",
    "language": "en",
    "term": "Confusion Matrix",
    "category": "Concept",
    "pronunciation": "/kənˈfjuːʒən ˈmeɪtrɪks/",
    "definition": "A table used to evaluate classification model performance by showing true vs predicted classes.",
    "explanation": "The confusion matrix displays true positives, false positives, true negatives, and false negatives. It enables calculation of metrics like precision, recall, F1-score, and accuracy.",
    "examples": ["Binary classification evaluation", "Multi-class performance analysis", "Model error analysis"],
    "relatedTerms": ["precision", "recall", "f1-score"]
  },
  {
    "slug": "confusion-matrix-it",
    "language": "it",
    "term": "Matrice di Confusione",
    "category": "Concetto",
    "pronunciation": "/kənˈfjuːʒən ˈmeɪtrɪks/",
    "definition": "Una tabella usata per valutare le performance di modelli di classificazione mostrando classi vere vs predette.",
    "explanation": "La matrice di confusione mostra true positives, false positives, true negatives e false negatives. Permette il calcolo di metriche come precision, recall, F1-score e accuracy.",
    "examples": ["Valutazione classificazione binaria", "Analisi performance multi-classe", "Analisi errori modello"],
    "relatedTerms": ["precision", "recall", "f1-score"]
  },
  {
    "slug": "precision-recall",
    "language": "en",
    "term": "Precision and Recall",
    "category": "Concept",
    "pronunciation": "/prɪˈsɪʒən ənd rɪˈkɔːl/",
    "definition": "Metrics for classification: Precision is correct positives / predicted positives; Recall is correct positives / actual positives.",
    "explanation": "Precision measures accuracy of positive predictions (avoiding false positives). Recall measures completeness (avoiding false negatives). The F1-score combines both into a single metric.",
    "examples": ["Medical diagnosis (high recall)", "Spam detection (high precision)", "Information retrieval"],
    "relatedTerms": ["confusion-matrix", "f1-score", "classification"]
  },
  {
    "slug": "precision-recall-it",
    "language": "it",
    "term": "Precision e Recall",
    "category": "Concetto",
    "pronunciation": "/prɪˈsɪʒən ənd rɪˈkɔːl/",
    "definition": "Metriche per classificazione: Precision è positivi corretti / positivi predetti; Recall è positivi corretti / positivi reali.",
    "explanation": "La precision misura l'accuratezza delle predizioni positive (evitando falsi positivi). Il recall misura la completezza (evitando falsi negativi). L'F1-score combina entrambi in una singola metrica.",
    "examples": ["Diagnosi medica (alto recall)", "Rilevamento spam (alta precision)", "Information retrieval"],
    "relatedTerms": ["confusion-matrix", "f1-score", "classification"]
  },
  {
    "slug": "clip",
    "language": "en",
    "term": "CLIP (Contrastive Language-Image Pre-training)",
    "category": "Model",
    "pronunciation": "/klɪp/",
    "definition": "A multimodal model trained to understand relationships between images and text.",
    "explanation": "CLIP learns joint embeddings of images and text through contrastive learning on 400M image-text pairs. It enables zero-shot image classification and powers many vision-language applications.",
    "examples": ["Zero-shot image classification", "Image search by text", "Text-to-image generation conditioning"],
    "relatedTerms": ["multimodal", "vision-language", "zero-shot-learning"]
  },
  {
    "slug": "clip-it",
    "language": "it",
    "term": "CLIP (Contrastive Language-Image Pre-training)",
    "category": "Modello",
    "pronunciation": "/klɪp/",
    "definition": "Un modello multimodale addestrato per comprendere relazioni tra immagini e testo.",
    "explanation": "CLIP apprende embedding congiunti di immagini e testo attraverso contrastive learning su 400M coppie immagine-testo. Permette classificazione immagini zero-shot e alimenta molte applicazioni vision-language.",
    "examples": ["Classificazione immagini zero-shot", "Ricerca immagini per testo", "Conditioning per text-to-image generation"],
    "relatedTerms": ["multimodal", "vision-language", "zero-shot-learning"]
  },
  {
    "slug": "supervised-learning",
    "language": "en",
    "term": "Supervised Learning",
    "category": "Paradigm",
    "pronunciation": "/ˈsuːpərvaɪzd ˈlɜːrnɪŋ/",
    "definition": "A machine learning paradigm where models learn from labeled training data with input-output pairs.",
    "explanation": "In supervised learning, algorithms learn to map inputs to outputs using labeled examples. The model is trained to minimize the difference between predictions and true labels. Common tasks include classification and regression.",
    "examples": ["Image classification with labels", "Spam email detection", "House price prediction"],
    "relatedTerms": ["classification", "regression", "labeled-data"]
  },
  {
    "slug": "supervised-learning-it",
    "language": "it",
    "term": "Apprendimento Supervisionato",
    "category": "Paradigma",
    "pronunciation": "/ˈsuːpərvaɪzd ˈlɜːrnɪŋ/",
    "definition": "Un paradigma di machine learning dove i modelli apprendono da dati di training etichettati con coppie input-output.",
    "explanation": "Nell'apprendimento supervisionato, gli algoritmi apprendono a mappare input su output usando esempi etichettati. Il modello è addestrato a minimizzare la differenza tra predizioni e etichette vere. Task comuni includono classificazione e regressione.",
    "examples": ["Classificazione immagini con etichette", "Rilevamento spam email", "Predizione prezzi case"],
    "relatedTerms": ["classification", "regression", "labeled-data"]
  },
  {
    "slug": "unsupervised-learning",
    "language": "en",
    "term": "Unsupervised Learning",
    "category": "Paradigm",
    "pronunciation": "/ˌʌnsupərˈvaɪzd ˈlɜːrnɪŋ/",
    "definition": "A machine learning paradigm where models find patterns in unlabeled data without explicit supervision.",
    "explanation": "Unsupervised learning discovers hidden structures, patterns, or representations in data without labels. Common techniques include clustering, dimensionality reduction, and anomaly detection.",
    "examples": ["Customer segmentation", "Topic modeling", "Anomaly detection"],
    "relatedTerms": ["clustering", "dimensionality-reduction", "unlabeled-data"]
  },
  {
    "slug": "unsupervised-learning-it",
    "language": "it",
    "term": "Apprendimento Non Supervisionato",
    "category": "Paradigma",
    "pronunciation": "/ˌʌnsupərˈvaɪzd ˈlɜːrnɪŋ/",
    "definition": "Un paradigma di machine learning dove i modelli trovano pattern in dati non etichettati senza supervisione esplicita.",
    "explanation": "L'apprendimento non supervisionato scopre strutture nascoste, pattern o rappresentazioni nei dati senza etichette. Tecniche comuni includono clustering, riduzione dimensionalità e rilevamento anomalie.",
    "examples": ["Segmentazione clienti", "Topic modeling", "Rilevamento anomalie"],
    "relatedTerms": ["clustering", "dimensionality-reduction", "unlabeled-data"]
  },
  {
    "slug": "classification",
    "language": "en",
    "term": "Classification",
    "category": "Task",
    "pronunciation": "/ˌklæsɪfɪˈkeɪʃən/",
    "definition": "A supervised learning task where the goal is to predict discrete class labels for input data.",
    "explanation": "Classification assigns inputs to predefined categories. Binary classification has two classes, multi-class has more. Output is typically a probability distribution over classes.",
    "examples": ["Email spam vs not spam", "Image classification (cat, dog, bird)", "Disease diagnosis"],
    "relatedTerms": ["supervised-learning", "softmax", "cross-entropy"]
  },
  {
    "slug": "classification-it",
    "language": "it",
    "term": "Classificazione",
    "category": "Task",
    "pronunciation": "/ˌklæsɪfɪˈkeɪʃən/",
    "definition": "Un task di apprendimento supervisionato dove l'obiettivo è predire etichette di classe discrete per dati di input.",
    "explanation": "La classificazione assegna input a categorie predefinite. La classificazione binaria ha due classi, multi-classe ne ha di più. L'output è tipicamente una distribuzione di probabilità sulle classi.",
    "examples": ["Email spam vs non spam", "Classificazione immagini (gatto, cane, uccello)", "Diagnosi malattie"],
    "relatedTerms": ["supervised-learning", "softmax", "cross-entropy"]
  },
  {
    "slug": "regression",
    "language": "en",
    "term": "Regression",
    "category": "Task",
    "pronunciation": "/rɪˈɡrɛʃən/",
    "definition": "A supervised learning task where the goal is to predict continuous numerical values.",
    "explanation": "Regression models learn relationships between input features and continuous outputs. Common types include linear regression, polynomial regression, and neural network regression.",
    "examples": ["House price prediction", "Stock price forecasting", "Temperature prediction"],
    "relatedTerms": ["supervised-learning", "mse", "linear-regression"]
  },
  {
    "slug": "regression-it",
    "language": "it",
    "term": "Regressione",
    "category": "Task",
    "pronunciation": "/rɪˈɡrɛʃən/",
    "definition": "Un task di apprendimento supervisionato dove l'obiettivo è predire valori numerici continui.",
    "explanation": "I modelli di regressione apprendono relazioni tra feature di input e output continui. Tipi comuni includono regressione lineare, regressione polinomiale e regressione con reti neurali.",
    "examples": ["Predizione prezzi case", "Previsione prezzi azioni", "Predizione temperatura"],
    "relatedTerms": ["supervised-learning", "mse", "linear-regression"]
  },
  {
    "slug": "clustering",
    "language": "en",
    "term": "Clustering",
    "category": "Task",
    "pronunciation": "/ˈklʌstərɪŋ/",
    "definition": "An unsupervised learning task that groups similar data points together based on their features.",
    "explanation": "Clustering algorithms partition data into groups (clusters) where items within a cluster are more similar to each other than to items in other clusters. Common algorithms include K-means, DBSCAN, and hierarchical clustering.",
    "examples": ["Customer segmentation", "Image compression", "Gene sequence analysis"],
    "relatedTerms": ["unsupervised-learning", "k-means", "similarity"]
  },
  {
    "slug": "clustering-it",
    "language": "it",
    "term": "Clustering",
    "category": "Task",
    "pronunciation": "/ˈklʌstərɪŋ/",
    "definition": "Un task di apprendimento non supervisionato che raggruppa punti dati simili insieme basandosi sulle loro feature.",
    "explanation": "Gli algoritmi di clustering partizionano i dati in gruppi (cluster) dove gli elementi all'interno di un cluster sono più simili tra loro che agli elementi di altri cluster. Algoritmi comuni includono K-means, DBSCAN e hierarchical clustering.",
    "examples": ["Segmentazione clienti", "Compressione immagini", "Analisi sequenze genetiche"],
    "relatedTerms": ["unsupervised-learning", "k-means", "similarity"]
  },
  {
    "slug": "dimensionality-reduction",
    "language": "en",
    "term": "Dimensionality Reduction",
    "category": "Technique",
    "pronunciation": "/dɪˌmɛnʃəˈnælɪti rɪˈdʌkʃən/",
    "definition": "Techniques to reduce the number of features in data while preserving important information.",
    "explanation": "Dimensionality reduction simplifies data by projecting it to lower dimensions. Benefits include faster computation, reduced storage, and better visualization. Common methods: PCA, t-SNE, UMAP.",
    "examples": ["PCA for visualization", "Feature compression", "Noise reduction"],
    "relatedTerms": ["pca", "t-sne", "feature-extraction"]
  },
  {
    "slug": "dimensionality-reduction-it",
    "language": "it",
    "term": "Riduzione Dimensionalità",
    "category": "Tecnica",
    "pronunciation": "/dɪˌmɛnʃəˈnælɪti rɪˈdʌkʃən/",
    "definition": "Tecniche per ridurre il numero di feature nei dati preservando informazioni importanti.",
    "explanation": "La riduzione della dimensionalità semplifica i dati proiettandoli in dimensioni inferiori. Benefici includono computazione più veloce, storage ridotto e migliore visualizzazione. Metodi comuni: PCA, t-SNE, UMAP.",
    "examples": ["PCA per visualizzazione", "Compressione feature", "Riduzione rumore"],
    "relatedTerms": ["pca", "t-sne", "feature-extraction"]
  },
  {
    "slug": "convolutional-layer",
    "language": "en",
    "term": "Convolutional Layer",
    "category": "Architecture",
    "pronunciation": "/ˌkɒnvəˈluːʃənəl ˈleɪər/",
    "definition": "A layer in CNNs that applies convolution operations to extract spatial features from input data.",
    "explanation": "Convolutional layers use learnable filters (kernels) that slide over input to detect local patterns. Multiple filters detect different features like edges, textures, or shapes. Output is a feature map.",
    "examples": ["Edge detection in images", "Pattern recognition", "Feature extraction in CNNs"],
    "relatedTerms": ["cnn", "filter", "feature-map"]
  },
  {
    "slug": "convolutional-layer-it",
    "language": "it",
    "term": "Layer Convoluzionale",
    "category": "Architettura",
    "pronunciation": "/ˌkɒnvəˈluːʃənəl ˈleɪər/",
    "definition": "Un layer nelle CNN che applica operazioni di convoluzione per estrarre feature spaziali dai dati di input.",
    "explanation": "I layer convoluzionali usano filtri apprendibili (kernel) che scorrono sull'input per rilevare pattern locali. Più filtri rilevano diverse feature come bordi, texture o forme. L'output è una feature map.",
    "examples": ["Rilevamento bordi in immagini", "Riconoscimento pattern", "Estrazione feature in CNN"],
    "relatedTerms": ["cnn", "filter", "feature-map"]
  },
  {
    "slug": "pooling-layer",
    "language": "en",
    "term": "Pooling Layer",
    "category": "Architecture",
    "pronunciation": "/ˈpuːlɪŋ ˈleɪər/",
    "definition": "A downsampling layer in CNNs that reduces spatial dimensions while retaining important features.",
    "explanation": "Pooling layers reduce feature map size by aggregating neighboring values. Max pooling takes the maximum value, average pooling takes the mean. This provides translation invariance and reduces computation.",
    "examples": ["Max pooling 2x2", "Average pooling", "Global average pooling"],
    "relatedTerms": ["cnn", "max-pooling", "downsampling"]
  },
  {
    "slug": "pooling-layer-it",
    "language": "it",
    "term": "Layer di Pooling",
    "category": "Architettura",
    "pronunciation": "/ˈpuːlɪŋ ˈleɪər/",
    "definition": "Un layer di downsampling nelle CNN che riduce le dimensioni spaziali mantenendo feature importanti.",
    "explanation": "I layer di pooling riducono la dimensione delle feature map aggregando valori vicini. Il max pooling prende il valore massimo, l'average pooling prende la media. Questo fornisce invarianza alla traslazione e riduce la computazione.",
    "examples": ["Max pooling 2x2", "Average pooling", "Global average pooling"],
    "relatedTerms": ["cnn", "max-pooling", "downsampling"]
  },
  {
    "slug": "recurrent-neural-network",
    "language": "en",
    "term": "RNN (Recurrent Neural Network)",
    "category": "Architecture",
    "pronunciation": "/rɪˈkʌrənt ˈnjʊərəl ˈnetwɜːrk/",
    "definition": "A neural network architecture designed for sequential data, with connections that loop back to previous states.",
    "explanation": "RNNs process sequences by maintaining a hidden state that captures information from previous time steps. They excel at tasks with temporal dependencies but suffer from vanishing gradients. LSTM and GRU address this issue.",
    "examples": ["Time series forecasting", "Text generation", "Speech recognition"],
    "relatedTerms": ["lstm", "gru", "sequence-modeling"]
  },
  {
    "slug": "recurrent-neural-network-it",
    "language": "it",
    "term": "RNN (Recurrent Neural Network)",
    "category": "Architettura",
    "pronunciation": "/rɪˈkʌrənt ˈnjʊərəl ˈnetwɜːrk/",
    "definition": "Un'architettura di rete neurale progettata per dati sequenziali, con connessioni che tornano a stati precedenti.",
    "explanation": "Le RNN processano sequenze mantenendo uno stato nascosto che cattura informazioni dai time step precedenti. Eccellono in task con dipendenze temporali ma soffrono di vanishing gradient. LSTM e GRU risolvono questo problema.",
    "examples": ["Previsione serie temporali", "Generazione testo", "Riconoscimento vocale"],
    "relatedTerms": ["lstm", "gru", "sequence-modeling"]
  },
  {
    "slug": "attention-weight",
    "language": "en",
    "term": "Attention Weight",
    "category": "Concept",
    "pronunciation": "/əˈtɛnʃən weɪt/",
    "definition": "A scalar value indicating how much focus to place on a specific part of the input when producing output.",
    "explanation": "Attention weights are computed through dot products of query and key vectors, followed by softmax normalization. Higher weights mean greater importance. They enable models to focus on relevant information dynamically.",
    "examples": ["Translation attention to source words", "Image captioning attention to regions", "Document summarization"],
    "relatedTerms": ["attention-mechanism", "self-attention", "softmax"]
  },
  {
    "slug": "attention-weight-it",
    "language": "it",
    "term": "Peso di Attenzione",
    "category": "Concetto",
    "pronunciation": "/əˈtɛnʃən weɪt/",
    "definition": "Un valore scalare che indica quanto focus porre su una specifica parte dell'input quando si produce l'output.",
    "explanation": "I pesi di attenzione sono calcolati attraverso prodotti scalari di vettori query e key, seguiti da normalizzazione softmax. Pesi più alti significano maggiore importanza. Permettono ai modelli di focalizzarsi dinamicamente su informazioni rilevanti.",
    "examples": ["Attenzione traduzione su parole sorgente", "Attenzione image captioning su regioni", "Summarization documenti"],
    "relatedTerms": ["attention-mechanism", "self-attention", "softmax"]
  },
  {
    "slug": "query-key-value",
    "language": "en",
    "term": "Query, Key, Value (QKV)",
    "category": "Concept",
    "pronunciation": "/ˈkwɪəri kiː ˈvæljuː/",
    "definition": "Three vectors used in attention mechanisms to compute weighted combinations of input elements.",
    "explanation": "In attention, inputs are projected to Query (what we're looking for), Key (what each element offers), and Value (actual content). Attention scores are computed as similarity between Query and Keys, then used to weight Values.",
    "examples": ["Transformer attention", "Self-attention computation", "Cross-attention in encoder-decoder"],
    "relatedTerms": ["attention-mechanism", "transformer", "dot-product"]
  },
  {
    "slug": "query-key-value-it",
    "language": "it",
    "term": "Query, Key, Value (QKV)",
    "category": "Concetto",
    "pronunciation": "/ˈkwɪəri kiː ˈvæljuː/",
    "definition": "Tre vettori usati nei meccanismi di attenzione per calcolare combinazioni pesate di elementi input.",
    "explanation": "Nell'attenzione, gli input sono proiettati in Query (cosa stiamo cercando), Key (cosa offre ogni elemento) e Value (contenuto effettivo). Gli score di attenzione sono calcolati come similarità tra Query e Key, poi usati per pesare i Value.",
    "examples": ["Attenzione transformer", "Computazione self-attention", "Cross-attention in encoder-decoder"],
    "relatedTerms": ["attention-mechanism", "transformer", "dot-product"]
  },
  {
    "slug": "positional-encoding",
    "language": "en",
    "term": "Positional Encoding",
    "category": "Technique",
    "pronunciation": "/pəˈzɪʃənəl ɪnˈkoʊdɪŋ/",
    "definition": "A technique to inject position information into transformer inputs since transformers lack inherent sequence order.",
    "explanation": "Positional encodings add unique patterns to each position in a sequence, allowing transformers to distinguish order. Common methods use sine/cosine functions or learned embeddings.",
    "examples": ["Sinusoidal positional encoding in original Transformer", "Learned positional embeddings", "Relative position encodings"],
    "relatedTerms": ["transformer", "sequence-modeling", "word-order"]
  },
  {
    "slug": "positional-encoding-it",
    "language": "it",
    "term": "Positional Encoding",
    "category": "Tecnica",
    "pronunciation": "/pəˈzɪʃənəl ɪnˈkoʊdɪŋ/",
    "definition": "Una tecnica per iniettare informazioni di posizione negli input dei transformer poiché mancano di ordine sequenziale intrinseco.",
    "explanation": "I positional encoding aggiungono pattern unici a ogni posizione in una sequenza, permettendo ai transformer di distinguere l'ordine. Metodi comuni usano funzioni seno/coseno o embedding appresi.",
    "examples": ["Positional encoding sinusoidale nel Transformer originale", "Positional embedding appresi", "Relative position encoding"],
    "relatedTerms": ["transformer", "sequence-modeling", "word-order"]
  },
  {
    "slug": "word-embedding",
    "language": "en",
    "term": "Word Embedding",
    "category": "Technique",
    "pronunciation": "/wɜːrd ɪmˈbɛdɪŋ/",
    "definition": "Dense vector representations of words that capture semantic and syntactic relationships.",
    "explanation": "Word embeddings map words to continuous vectors where semantically similar words are close in vector space. Learned from large corpora, they enable transfer learning in NLP. Popular methods: Word2Vec, GloVe, FastText.",
    "examples": ["Word2Vec embeddings", "GloVe vectors", "Contextual embeddings from BERT"],
    "relatedTerms": ["embedding", "word2vec", "semantic-similarity"]
  },
  {
    "slug": "word-embedding-it",
    "language": "it",
    "term": "Word Embedding",
    "category": "Tecnica",
    "pronunciation": "/wɜːrd ɪmˈbɛdɪŋ/",
    "definition": "Rappresentazioni vettoriali dense di parole che catturano relazioni semantiche e sintattiche.",
    "explanation": "I word embedding mappano parole a vettori continui dove parole semanticamente simili sono vicine nello spazio vettoriale. Appresi da grandi corpus, permettono transfer learning in NLP. Metodi popolari: Word2Vec, GloVe, FastText.",
    "examples": ["Embedding Word2Vec", "Vettori GloVe", "Embedding contestuali da BERT"],
    "relatedTerms": ["embedding", "word2vec", "semantic-similarity"]
  },
  {
    "slug": "early-stopping",
    "language": "en",
    "term": "Early Stopping",
    "category": "Technique",
    "pronunciation": "/ˈɜːrli ˈstɒpɪŋ/",
    "definition": "A regularization technique that stops training when validation performance stops improving.",
    "explanation": "Early stopping monitors validation loss during training and stops when it hasn't improved for a set number of epochs (patience). This prevents overfitting by stopping before the model memorizes training data.",
    "examples": ["Stop training at epoch 50 when validation loss plateaus", "Patience of 10 epochs", "Restore best weights"],
    "relatedTerms": ["overfitting", "validation-set", "regularization"]
  },
  {
    "slug": "early-stopping-it",
    "language": "it",
    "term": "Early Stopping",
    "category": "Tecnica",
    "pronunciation": "/ˈɜːrli ˈstɒpɪŋ/",
    "definition": "Una tecnica di regolarizzazione che ferma il training quando le performance su validation smettono di migliorare.",
    "explanation": "L'early stopping monitora la validation loss durante il training e si ferma quando non migliora per un numero stabilito di epoche (pazienza). Questo previene overfitting fermandosi prima che il modello memorizzi i dati di training.",
    "examples": ["Fermare training a epoca 50 quando validation loss si stabilizza", "Pazienza di 10 epoche", "Ripristino best weights"],
    "relatedTerms": ["overfitting", "validation-set", "regularization"]
  },
  {
    "slug": "data-augmentation-en",
    "language": "en",
    "term": "Data Augmentation",
    "category": "Technique",
    "pronunciation": "/ˈdeɪtə ˌɔːɡmɛnˈteɪʃən/",
    "definition": "Techniques to artificially increase training data size by creating modified versions of existing data.",
    "explanation": "Data augmentation creates variations of training examples through transformations like rotation, flipping, cropping (images), synonym replacement (text), or noise injection (audio). Improves generalization and reduces overfitting.",
    "examples": ["Image rotation and flipping", "Text back-translation", "Audio pitch shifting"],
    "relatedTerms": ["overfitting", "training", "generalization"]
  },
  {
    "slug": "vanishing-gradient",
    "language": "en",
    "term": "Vanishing Gradient",
    "category": "Concept",
    "pronunciation": "/ˈvænɪʃɪŋ ˈɡreɪdiənt/",
    "definition": "A problem in deep networks where gradients become extremely small, preventing effective learning in early layers.",
    "explanation": "Vanishing gradients occur when repeated multiplication of small derivatives (< 1) makes gradients exponentially smaller in backpropagation. Common in deep RNNs and networks with sigmoid activations. Solutions: ReLU, LSTM, ResNet.",
    "examples": ["Deep RNNs failing to learn long dependencies", "Sigmoid activation in deep networks", "Pre-ResNet very deep networks"],
    "relatedTerms": ["backpropagation", "gradient-descent", "relu"]
  },
  {
    "slug": "vanishing-gradient-it",
    "language": "it",
    "term": "Vanishing Gradient",
    "category": "Concetto",
    "pronunciation": "/ˈvænɪʃɪŋ ˈɡreɪdiənt/",
    "definition": "Un problema nelle reti profonde dove i gradienti diventano estremamente piccoli, impedendo apprendimento efficace nei layer iniziali.",
    "explanation": "Il vanishing gradient si verifica quando la moltiplicazione ripetuta di derivate piccole (< 1) rende i gradienti esponenzialmente più piccoli nella backpropagation. Comune in RNN profonde e reti con attivazioni sigmoid. Soluzioni: ReLU, LSTM, ResNet.",
    "examples": ["RNN profonde che falliscono nell'apprendere dipendenze lunghe", "Attivazione sigmoid in reti profonde", "Reti molto profonde pre-ResNet"],
    "relatedTerms": ["backpropagation", "gradient-descent", "relu"]
  },
  {
    "slug": "exploding-gradient",
    "language": "en",
    "term": "Exploding Gradient",
    "category": "Concept",
    "pronunciation": "/ɪkˈsploʊdɪŋ ˈɡreɪdiənt/",
    "definition": "A problem where gradients become extremely large during training, causing unstable updates and divergence.",
    "explanation": "Exploding gradients occur when repeated multiplication of large derivatives (> 1) makes gradients exponentially larger. This causes massive weight updates that destabilize training. Solutions: gradient clipping, proper initialization, batch normalization.",
    "examples": ["RNN training divergence", "NaN values in weights", "Oscillating loss"],
    "relatedTerms": ["gradient-clipping", "backpropagation", "training"]
  },
  {
    "slug": "exploding-gradient-it",
    "language": "it",
    "term": "Exploding Gradient",
    "category": "Concetto",
    "pronunciation": "/ɪkˈsploʊdɪŋ ˈɡreɪdiənt/",
    "definition": "Un problema dove i gradienti diventano estremamente grandi durante il training, causando aggiornamenti instabili e divergenza.",
    "explanation": "L'exploding gradient si verifica quando la moltiplicazione ripetuta di derivate grandi (> 1) rende i gradienti esponenzialmente più grandi. Questo causa aggiornamenti massicci dei pesi che destabilizzano il training. Soluzioni: gradient clipping, inizializzazione corretta, batch normalization.",
    "examples": ["Divergenza training RNN", "Valori NaN nei pesi", "Loss oscillante"],
    "relatedTerms": ["gradient-clipping", "backpropagation", "training"]
  },
  {
    "slug": "weight-initialization",
    "language": "en",
    "term": "Weight Initialization",
    "category": "Technique",
    "pronunciation": "/weɪt ɪˌnɪʃəlaɪˈzeɪʃən/",
    "definition": "Methods for setting initial values of neural network weights before training begins.",
    "explanation": "Proper initialization is crucial for effective training. Random initialization breaks symmetry. Methods like Xavier/Glorot (for tanh/sigmoid) and He initialization (for ReLU) ensure gradients flow well initially.",
    "examples": ["Xavier initialization for tanh layers", "He initialization for ReLU layers", "Zero initialization for biases"],
    "relatedTerms": ["training", "vanishing-gradient", "neural-network"]
  },
  {
    "slug": "weight-initialization-it",
    "language": "it",
    "term": "Inizializzazione Pesi",
    "category": "Tecnica",
    "pronunciation": "/weɪt ɪˌnɪʃəlaɪˈzeɪʃən/",
    "definition": "Metodi per impostare valori iniziali dei pesi della rete neurale prima dell'inizio del training.",
    "explanation": "Un'inizializzazione corretta è cruciale per un training efficace. L'inizializzazione random rompe la simmetria. Metodi come Xavier/Glorot (per tanh/sigmoid) e He initialization (per ReLU) assicurano che i gradienti fluiscano bene inizialmente.",
    "examples": ["Inizializzazione Xavier per layer tanh", "Inizializzazione He per layer ReLU", "Inizializzazione zero per bias"],
    "relatedTerms": ["training", "vanishing-gradient", "neural-network"]
  },
  {
    "slug": "momentum",
    "language": "en",
    "term": "Momentum",
    "category": "Technique",
    "pronunciation": "/moʊˈmɛntəm/",
    "definition": "An optimization technique that accelerates gradient descent by accumulating a velocity vector in directions of persistent reduction in the loss.",
    "explanation": "Momentum helps navigate ravines and accelerates convergence. It adds a fraction of the previous update vector to the current one, smoothing optimization trajectory and reducing oscillations.",
    "examples": ["SGD with momentum 0.9", "Nesterov momentum", "Accelerating convergence"],
    "relatedTerms": ["gradient-descent", "optimization", "sgd"]
  },
  {
    "slug": "momentum-it",
    "language": "it",
    "term": "Momentum",
    "category": "Tecnica",
    "pronunciation": "/moʊˈmɛntəm/",
    "definition": "Una tecnica di ottimizzazione che accelera gradient descent accumulando un vettore velocità in direzioni di riduzione persistente della loss.",
    "explanation": "Il momentum aiuta a navigare i dirupi e accelera la convergenza. Aggiunge una frazione del vettore di aggiornamento precedente a quello corrente, lisciando la traiettoria di ottimizzazione e riducendo oscillazioni.",
    "examples": ["SGD con momentum 0.9", "Nesterov momentum", "Accelerazione convergenza"],
    "relatedTerms": ["gradient-descent", "optimization", "sgd"]
  },
  {
    "slug": "learning-rate-schedule",
    "language": "en",
    "term": "Learning Rate Schedule",
    "category": "Technique",
    "pronunciation": "/ˈlɜːrnɪŋ reɪt ˈʃɛdjuːl/",
    "definition": "A strategy for adjusting the learning rate during training to improve convergence and performance.",
    "explanation": "Learning rate schedules reduce the learning rate over time. Common strategies include step decay, exponential decay, cosine annealing, and warm restarts. This helps fine-tune weights as training progresses.",
    "examples": ["Reduce LR on plateau", "Cosine annealing", "Warm-up + decay schedule"],
    "relatedTerms": ["learning-rate", "optimization", "training"]
  },
  {
    "slug": "learning-rate-schedule-it",
    "language": "it",
    "term": "Learning Rate Schedule",
    "category": "Tecnica",
    "pronunciation": "/ˈlɜːrnɪŋ reɪt ˈʃɛdjuːl/",
    "definition": "Una strategia per aggiustare il learning rate durante il training per migliorare convergenza e performance.",
    "explanation": "Gli schedule del learning rate riducono il learning rate nel tempo. Strategie comuni includono step decay, exponential decay, cosine annealing e warm restart. Questo aiuta a fine-tune i pesi mentre il training progredisce.",
    "examples": ["Riduzione LR su plateau", "Cosine annealing", "Schedule warm-up + decay"],
    "relatedTerms": ["learning-rate", "optimization", "training"]
  },
  {
    "slug": "batch-gradient-descent",
    "language": "en",
    "term": "Batch Gradient Descent",
    "category": "Algorithm",
    "pronunciation": "/bætʃ ˈɡreɪdiənt dɪˈsɛnt/",
    "definition": "A gradient descent variant that computes gradients using the entire training dataset in each iteration.",
    "explanation": "Batch GD provides stable, accurate gradient estimates but is computationally expensive for large datasets. It guarantees convergence to global minimum for convex problems.",
    "examples": ["Full batch training on small datasets", "Theoretical analysis", "Deterministic optimization"],
    "relatedTerms": ["gradient-descent", "sgd", "mini-batch"]
  },
  {
    "slug": "batch-gradient-descent-it",
    "language": "it",
    "term": "Batch Gradient Descent",
    "category": "Algoritmo",
    "pronunciation": "/bætʃ ˈɡreɪdiənt dɪˈsɛnt/",
    "definition": "Una variante di gradient descent che calcola i gradienti usando l'intero dataset di training in ogni iterazione.",
    "explanation": "Il Batch GD fornisce stime di gradiente stabili e accurate ma è computazionalmente costoso per dataset grandi. Garantisce convergenza al minimo globale per problemi convessi.",
    "examples": ["Training full batch su dataset piccoli", "Analisi teorica", "Ottimizzazione deterministica"],
    "relatedTerms": ["gradient-descent", "sgd", "mini-batch"]
  },
  {
    "slug": "sgd",
    "language": "en",
    "term": "SGD (Stochastic Gradient Descent)",
    "category": "Algorithm",
    "pronunciation": "/stoʊˈkæstɪk ˈɡreɪdiənt dɪˈsɛnt/",
    "definition": "A gradient descent variant that updates weights using gradients from a single random training example at a time.",
    "explanation": "SGD is faster and enables online learning but has noisy gradients. The noise can help escape local minima. Mini-batch SGD balances efficiency and gradient quality.",
    "examples": ["Online learning", "Large-scale training", "Escaping local minima"],
    "relatedTerms": ["gradient-descent", "mini-batch", "optimization"]
  },
  {
    "slug": "sgd-it",
    "language": "it",
    "term": "SGD (Stochastic Gradient Descent)",
    "category": "Algoritmo",
    "pronunciation": "/stoʊˈkæstɪk ˈɡreɪdiənt dɪˈsɛnt/",
    "definition": "Una variante di gradient descent che aggiorna i pesi usando gradienti da un singolo esempio di training random alla volta.",
    "explanation": "SGD è più veloce e permette online learning ma ha gradienti rumorosi. Il rumore può aiutare a scappare da minimi locali. Il mini-batch SGD bilancia efficienza e qualità dei gradienti.",
    "examples": ["Online learning", "Training large-scale", "Scappare da minimi locali"],
    "relatedTerms": ["gradient-descent", "mini-batch", "optimization"]
  },
  {
    "slug": "convolution",
    "language": "en",
    "term": "Convolution",
    "category": "Concept",
    "pronunciation": "/ˌkɒnvəˈluːʃən/",
    "definition": "A mathematical operation that slides a filter/kernel over input data to extract features.",
    "explanation": "Convolution computes dot products between the filter and local regions of input. The filter moves with a stride, producing a feature map. Key operation in CNNs for spatial feature extraction.",
    "examples": ["2D convolution for images", "1D convolution for sequences", "Edge detection filters"],
    "relatedTerms": ["cnn", "filter", "feature-map"]
  },
  {
    "slug": "convolution-it",
    "language": "it",
    "term": "Convoluzione",
    "category": "Concetto",
    "pronunciation": "/ˌkɒnvəˈluːʃən/",
    "definition": "Un'operazione matematica che fa scorrere un filtro/kernel sui dati di input per estrarre feature.",
    "explanation": "La convoluzione calcola prodotti scalari tra il filtro e regioni locali dell'input. Il filtro si muove con uno stride, producendo una feature map. Operazione chiave nelle CNN per estrazione feature spaziali.",
    "examples": ["Convoluzione 2D per immagini", "Convoluzione 1D per sequenze", "Filtri edge detection"],
    "relatedTerms": ["cnn", "filter", "feature-map"]
  },
  {
    "slug": "stride",
    "language": "en",
    "term": "Stride",
    "category": "Concept",
    "pronunciation": "/straɪd/",
    "definition": "The number of pixels by which a filter moves across the input during convolution or pooling operations.",
    "explanation": "Stride controls output size - larger strides produce smaller outputs. Stride 1 moves one pixel at a time, stride 2 skips every other position. Affects receptive field and computational cost.",
    "examples": ["Stride 1 for detailed features", "Stride 2 for downsampling", "Stride > 1 reduces computation"],
    "relatedTerms": ["convolution", "pooling", "receptive-field"]
  },
  {
    "slug": "stride-it",
    "language": "it",
    "term": "Stride",
    "category": "Concetto",
    "pronunciation": "/straɪd/",
    "definition": "Il numero di pixel di cui un filtro si muove attraverso l'input durante operazioni di convoluzione o pooling.",
    "explanation": "Lo stride controlla la dimensione output - stride più grandi producono output più piccoli. Stride 1 si muove un pixel alla volta, stride 2 salta ogni altra posizione. Influenza receptive field e costo computazionale.",
    "examples": ["Stride 1 per feature dettagliate", "Stride 2 per downsampling", "Stride > 1 riduce computazione"],
    "relatedTerms": ["convolution", "pooling", "receptive-field"]
  },
  {
    "slug": "padding",
    "language": "en",
    "term": "Padding",
    "category": "Technique",
    "pronunciation": "/ˈpædɪŋ/",
    "definition": "Adding extra pixels around the border of input data to control output size in convolution operations.",
    "explanation": "Padding preserves spatial dimensions and prevents information loss at borders. 'Valid' padding = no padding, 'Same' padding = output size matches input. Common: zero-padding.",
    "examples": ["Zero padding for same output size", "Valid padding for reduction", "Preserving border information"],
    "relatedTerms": ["convolution", "cnn", "output-size"]
  },
  {
    "slug": "padding-it",
    "language": "it",
    "term": "Padding",
    "category": "Tecnica",
    "pronunciation": "/ˈpædɪŋ/",
    "definition": "Aggiungere pixel extra attorno al bordo dei dati di input per controllare la dimensione output nelle operazioni di convoluzione.",
    "explanation": "Il padding preserva le dimensioni spaziali e previene perdita di informazione ai bordi. 'Valid' padding = nessun padding, 'Same' padding = dimensione output uguale a input. Comune: zero-padding.",
    "examples": ["Zero padding per stessa dimensione output", "Valid padding per riduzione", "Preservare informazione bordi"],
    "relatedTerms": ["convolution", "cnn", "output-size"]
  },
  {
    "slug": "receptive-field",
    "language": "en",
    "term": "Receptive Field",
    "category": "Concept",
    "pronunciation": "/rɪˈsɛptɪv fiːld/",
    "definition": "The region of input space that affects a particular neuron's activation in a neural network.",
    "explanation": "In CNNs, receptive field grows with each layer. Deeper neurons see larger input regions. Determined by filter size, stride, and network depth. Critical for understanding what information neurons can access.",
    "examples": ["3x3 filter has 3x3 receptive field", "Deeper layers see larger regions", "Dilated convolutions increase receptive field"],
    "relatedTerms": ["cnn", "convolution", "feature-hierarchy"]
  },
  {
    "slug": "receptive-field-it",
    "language": "it",
    "term": "Receptive Field",
    "category": "Concetto",
    "pronunciation": "/rɪˈsɛptɪv fiːld/",
    "definition": "La regione dello spazio di input che influenza l'attivazione di un particolare neurone in una rete neurale.",
    "explanation": "Nelle CNN, il receptive field cresce con ogni layer. I neuroni più profondi vedono regioni input più grandi. Determinato da dimensione filtro, stride e profondità rete. Critico per capire quali informazioni i neuroni possono accedere.",
    "examples": ["Filtro 3x3 ha receptive field 3x3", "Layer più profondi vedono regioni più grandi", "Convoluzioni dilatate aumentano receptive field"],
    "relatedTerms": ["cnn", "convolution", "feature-hierarchy"]
  },
  {
    "slug": "feature-map",
    "language": "en",
    "term": "Feature Map",
    "category": "Concept",
    "pronunciation": "/ˈfiːtʃər mæp/",
    "definition": "The output of applying a convolution filter to an input, representing detected features.",
    "explanation": "Each filter produces one feature map showing where specific features (edges, textures, patterns) are detected. Multiple filters produce multiple feature maps, each capturing different aspects of the input.",
    "examples": ["Edge detection feature map", "Texture feature maps", "High-level semantic features"],
    "relatedTerms": ["convolution", "cnn", "filter"]
  },
  {
    "slug": "feature-map-it",
    "language": "it",
    "term": "Feature Map",
    "category": "Concetto",
    "pronunciation": "/ˈfiːtʃər mæp/",
    "definition": "L'output dell'applicazione di un filtro di convoluzione a un input, rappresentando feature rilevate.",
    "explanation": "Ogni filtro produce una feature map mostrando dove feature specifiche (bordi, texture, pattern) sono rilevate. Più filtri producono più feature map, ognuna catturando aspetti diversi dell'input.",
    "examples": ["Feature map edge detection", "Feature map texture", "Feature semantiche alto livello"],
    "relatedTerms": ["convolution", "cnn", "filter"]
  },
  {
    "slug": "filters-kernels",
    "language": "en",
    "term": "Filter / Kernel",
    "category": "Concept",
    "pronunciation": "/ˈfɪltər ˈkɜːrnəl/",
    "definition": "A small matrix of learnable weights that slides over input during convolution to detect specific features.",
    "explanation": "Filters are learned during training to detect useful features. Common sizes: 3x3, 5x5, 7x7. Multiple filters extract different features. The term 'kernel' is used interchangeably with filter.",
    "examples": ["3x3 edge detection filter", "5x5 Gaussian blur kernel", "Learned feature detectors"],
    "relatedTerms": ["convolution", "cnn", "feature-map"]
  },
  {
    "slug": "filters-kernels-it",
    "language": "it",
    "term": "Filtro / Kernel",
    "category": "Concetto",
    "pronunciation": "/ˈfɪltər ˈkɜːrnəl/",
    "definition": "Una piccola matrice di pesi apprendibili che scorre sull'input durante la convoluzione per rilevare feature specifiche.",
    "explanation": "I filtri sono appresi durante il training per rilevare feature utili. Dimensioni comuni: 3x3, 5x5, 7x7. Più filtri estraggono feature diverse. Il termine 'kernel' è usato in modo intercambiabile con filtro.",
    "examples": ["Filtro 3x3 edge detection", "Kernel 5x5 Gaussian blur", "Rilevatori feature appresi"],
    "relatedTerms": ["convolution", "cnn", "feature-map"]
  },
  {
    "slug": "max-pooling",
    "language": "en",
    "term": "Max Pooling",
    "category": "Technique",
    "pronunciation": "/mæks ˈpuːlɪŋ/",
    "definition": "A pooling operation that takes the maximum value from each window of the feature map.",
    "explanation": "Max pooling reduces spatial dimensions while preserving the strongest activations. Provides translation invariance and reduces computation. Most common pooling method in CNNs.",
    "examples": ["2x2 max pooling", "Downsampling by factor 2", "Spatial invariance"],
    "relatedTerms": ["pooling-layer", "cnn", "downsampling"]
  },
  {
    "slug": "max-pooling-it",
    "language": "it",
    "term": "Max Pooling",
    "category": "Tecnica",
    "pronunciation": "/mæks ˈpuːlɪŋ/",
    "definition": "Un'operazione di pooling che prende il valore massimo da ogni finestra della feature map.",
    "explanation": "Il max pooling riduce le dimensioni spaziali preservando le attivazioni più forti. Fornisce invarianza alla traslazione e riduce la computazione. Metodo di pooling più comune nelle CNN.",
    "examples": ["Max pooling 2x2", "Downsampling di fattore 2", "Invarianza spaziale"],
    "relatedTerms": ["pooling-layer", "cnn", "downsampling"]
  },
  {
    "slug": "average-pooling",
    "language": "en",
    "term": "Average Pooling",
    "category": "Technique",
    "pronunciation": "/ˈævərɪdʒ ˈpuːlɪŋ/",
    "definition": "A pooling operation that computes the average value from each window of the feature map.",
    "explanation": "Average pooling downsamples by taking mean values, preserving more information than max pooling but potentially losing strong activations. Often used as final layer in classification networks.",
    "examples": ["Global average pooling", "Smooth downsampling", "Reducing overfitting"],
    "relatedTerms": ["pooling-layer", "max-pooling", "global-pooling"]
  },
  {
    "slug": "average-pooling-it",
    "language": "it",
    "term": "Average Pooling",
    "category": "Tecnica",
    "pronunciation": "/ˈævərɪdʒ ˈpuːlɪŋ/",
    "definition": "Un'operazione di pooling che calcola il valore medio da ogni finestra della feature map.",
    "explanation": "L'average pooling effettua downsampling prendendo valori medi, preservando più informazione del max pooling ma potenzialmente perdendo attivazioni forti. Spesso usato come layer finale nelle reti di classificazione.",
    "examples": ["Global average pooling", "Downsampling smooth", "Riduzione overfitting"],
    "relatedTerms": ["pooling-layer", "max-pooling", "global-pooling"]
  },
  {
    "slug": "fully-connected-layer",
    "language": "en",
    "term": "Fully Connected Layer",
    "category": "Architecture",
    "pronunciation": "/ˈfʊli kəˈnɛktɪd ˈleɪər/",
    "definition": "A neural network layer where every neuron is connected to every neuron in the previous and next layers.",
    "explanation": "Also called dense layers, they perform classification or regression after feature extraction. Each connection has a learnable weight. Common at the end of CNNs for final decision making.",
    "examples": ["Classification layer in CNNs", "MLP hidden layers", "Output layer"],
    "relatedTerms": ["dense-layer", "neural-network", "mlp"]
  },
  {
    "slug": "fully-connected-layer-it",
    "language": "it",
    "term": "Layer Fully Connected",
    "category": "Architettura",
    "pronunciation": "/ˈfʊli kəˈnɛktɪd ˈleɪər/",
    "definition": "Un layer di rete neurale dove ogni neurone è connesso a ogni neurone nei layer precedente e successivo.",
    "explanation": "Chiamati anche layer densi, eseguono classificazione o regressione dopo estrazione feature. Ogni connessione ha un peso apprendibile. Comuni alla fine delle CNN per decision making finale.",
    "examples": ["Layer classificazione in CNN", "Layer nascosti MLP", "Layer output"],
    "relatedTerms": ["dense-layer", "neural-network", "mlp"]
  },
  {
    "slug": "hidden-layer",
    "language": "en",
    "term": "Hidden Layer",
    "category": "Architecture",
    "pronunciation": "/ˈhɪdən ˈleɪər/",
    "definition": "Layers in a neural network between input and output that learn intermediate representations.",
    "explanation": "Hidden layers extract increasingly abstract features. Deep networks have many hidden layers. The term 'hidden' refers to not being directly observable as input or output.",
    "examples": ["Multiple hidden layers in deep networks", "Feature extraction layers", "Representation learning"],
    "relatedTerms": ["neural-network", "deep-learning", "layer"]
  },
  {
    "slug": "hidden-layer-it",
    "language": "it",
    "term": "Layer Nascosto",
    "category": "Architettura",
    "pronunciation": "/ˈhɪdən ˈleɪər/",
    "definition": "Layer in una rete neurale tra input e output che apprendono rappresentazioni intermedie.",
    "explanation": "I layer nascosti estraggono feature sempre più astratte. Le reti profonde hanno molti layer nascosti. Il termine 'nascosto' si riferisce al non essere direttamente osservabile come input o output.",
    "examples": ["Molteplici layer nascosti in reti profonde", "Layer estrazione feature", "Representation learning"],
    "relatedTerms": ["neural-network", "deep-learning", "layer"]
  },
  {
    "slug": "activation-layer",
    "language": "en",
    "term": "Activation Layer",
    "category": "Architecture",
    "pronunciation": "/ˌæktɪˈveɪʃən ˈleɪər/",
    "definition": "A layer that applies a non-linear activation function element-wise to its input.",
    "explanation": "Activation layers introduce non-linearity, enabling networks to learn complex patterns. Often placed after linear transformations (convolution, dense layers). Common: ReLU, sigmoid, tanh layers.",
    "examples": ["ReLU activation layer", "Sigmoid output layer", "Tanh hidden layer"],
    "relatedTerms": ["activation-function", "relu", "non-linearity"]
  },
  {
    "slug": "activation-layer-it",
    "language": "it",
    "term": "Layer di Attivazione",
    "category": "Architettura",
    "pronunciation": "/ˌæktɪˈveɪʃən ˈleɪər/",
    "definition": "Un layer che applica una funzione di attivazione non lineare element-wise al suo input.",
    "explanation": "I layer di attivazione introducono non-linearità, permettendo alle reti di apprendere pattern complessi. Spesso posti dopo trasformazioni lineari (convoluzione, layer densi). Comuni: layer ReLU, sigmoid, tanh.",
    "examples": ["Layer attivazione ReLU", "Layer output sigmoid", "Layer nascosto tanh"],
    "relatedTerms": ["activation-function", "relu", "non-linearity"]
  },
  {
    "slug": "input-layer",
    "language": "en",
    "term": "Input Layer",
    "category": "Architecture",
    "pronunciation": "/ˈɪnpʊt ˈleɪər/",
    "definition": "The first layer of a neural network that receives raw input data.",
    "explanation": "The input layer has one neuron per input feature. It doesn't perform computation, just passes data to the first hidden layer. Its size is determined by input data dimensions.",
    "examples": ["784 neurons for 28x28 images", "Input shape specification", "Data entry point"],
    "relatedTerms": ["neural-network", "input-shape", "architecture"]
  },
  {
    "slug": "input-layer-it",
    "language": "it",
    "term": "Layer di Input",
    "category": "Architettura",
    "pronunciation": "/ˈɪnpʊt ˈleɪər/",
    "definition": "Il primo layer di una rete neurale che riceve i dati di input grezzi.",
    "explanation": "Il layer di input ha un neurone per feature di input. Non esegue computazione, passa solo i dati al primo layer nascosto. La sua dimensione è determinata dalle dimensioni dei dati di input.",
    "examples": ["784 neuroni per immagini 28x28", "Specifica input shape", "Punto di entrata dati"],
    "relatedTerms": ["neural-network", "input-shape", "architecture"]
  },
  {
    "slug": "output-layer",
    "language": "en",
    "term": "Output Layer",
    "category": "Architecture",
    "pronunciation": "/ˈaʊtpʊt ˈleɪər/",
    "definition": "The final layer of a neural network that produces predictions or outputs.",
    "explanation": "Output layer size matches the number of target classes (classification) or output dimensions (regression). Uses appropriate activation: softmax for multi-class, sigmoid for binary, linear for regression.",
    "examples": ["10 neurons for digit classification", "1 neuron for binary classification", "Multiple outputs for regression"],
    "relatedTerms": ["neural-network", "softmax", "prediction"]
  },
  {
    "slug": "output-layer-it",
    "language": "it",
    "term": "Layer di Output",
    "category": "Architettura",
    "pronunciation": "/ˈaʊtpʊt ˈleɪər/",
    "definition": "Il layer finale di una rete neurale che produce predizioni o output.",
    "explanation": "La dimensione del layer di output corrisponde al numero di classi target (classificazione) o dimensioni output (regressione). Usa attivazione appropriata: softmax per multi-classe, sigmoid per binaria, lineare per regressione.",
    "examples": ["10 neuroni per classificazione cifre", "1 neurone per classificazione binaria", "Output multipli per regressione"],
    "relatedTerms": ["neural-network", "softmax", "prediction"]
  },
  {
    "slug": "bias-term",
    "language": "en",
    "term": "Bias Term",
    "category": "Concept",
    "pronunciation": "/ˈbaɪəs tɜːrm/",
    "definition": "An additional learnable parameter in neural networks that allows shifting the activation function.",
    "explanation": "Bias terms enable neurons to activate even when all inputs are zero. Each neuron typically has one bias. They increase model flexibility and are crucial for fitting non-zero-centered data.",
    "examples": ["Linear transformation: y = Wx + b", "Bias in each layer", "Offset parameter"],
    "relatedTerms": ["neural-network", "weight", "parameter"]
  },
  {
    "slug": "bias-term-it",
    "language": "it",
    "term": "Termine di Bias",
    "category": "Concetto",
    "pronunciation": "/ˈbaɪəs tɜːrm/",
    "definition": "Un parametro apprendibile addizionale nelle reti neurali che permette di shiftare la funzione di attivazione.",
    "explanation": "I termini di bias permettono ai neuroni di attivarsi anche quando tutti gli input sono zero. Ogni neurone ha tipicamente un bias. Aumentano la flessibilità del modello e sono cruciali per fittare dati non centrati sullo zero.",
    "examples": ["Trasformazione lineare: y = Wx + b", "Bias in ogni layer", "Parametro offset"],
    "relatedTerms": ["neural-network", "weight", "parameter"]
  },
  {
    "slug": "f1-score",
    "language": "en",
    "term": "F1 Score",
    "category": "Metric",
    "definition": "The harmonic mean of precision and recall, providing a single balanced metric.",
    "explanation": "F1 = 2 * (precision * recall) / (precision + recall). Useful for imbalanced datasets. Weighs precision and recall equally.",
    "examples": ["Binary classification evaluation", "Imbalanced datasets", "Medical diagnosis metrics"],
    "relatedTerms": ["precision-recall", "classification", "confusion-matrix"]
  },
  {
    "slug": "f1-score-it",
    "language": "it",
    "term": "F1 Score",
    "category": "Metrica",
    "definition": "La media armonica di precision e recall, fornendo una singola metrica bilanciata.",
    "explanation": "F1 = 2 * (precision * recall) / (precision + recall). Utile per dataset sbilanciati. Pesa precision e recall equamente.",
    "examples": ["Valutazione classificazione binaria", "Dataset sbilanciati", "Metriche diagnosi medica"],
    "relatedTerms": ["precision-recall", "classification", "confusion-matrix"]
  },
  {
    "slug": "auc-roc",
    "language": "en",
    "term": "AUC-ROC",
    "category": "Metric",
    "definition": "Area under ROC curve, measuring binary classifier quality across all thresholds.",
    "explanation": "ROC plots true positive rate vs false positive rate. AUC = 1 is perfect, 0.5 is random. Threshold-independent metric.",
    "examples": ["Binary classification evaluation", "Model comparison", "Imbalanced classes"],
    "relatedTerms": ["classification", "threshold", "binary-classifier"]
  },
  {
    "slug": "auc-roc-it",
    "language": "it",
    "term": "AUC-ROC",
    "category": "Metrica",
    "definition": "Area sotto curva ROC, misurando qualità classificatori binari attraverso tutte le soglie.",
    "explanation": "ROC plotta true positive rate vs false positive rate. AUC = 1 è perfetto, 0.5 è random. Metrica threshold-independent.",
    "examples": ["Valutazione classificazione binaria", "Comparazione modelli", "Classi sbilanciate"],
    "relatedTerms": ["classification", "threshold", "binary-classifier"]
  },
  {
    "slug": "cross-validation",
    "language": "en",
    "term": "Cross-Validation",
    "category": "Technique",
    "definition": "Resampling technique evaluating model performance by splitting data into multiple train-test folds.",
    "explanation": "K-fold CV divides data into k parts, trains on k-1 and tests on 1, repeating k times. Provides robust performance estimates.",
    "examples": ["5-fold cross-validation", "Leave-one-out CV", "Stratified k-fold"],
    "relatedTerms": ["validation-set", "overfitting", "model-evaluation"]
  },
  {
    "slug": "cross-validation-it",
    "language": "it",
    "term": "Cross-Validation",
    "category": "Tecnica",
    "definition": "Tecnica di resampling per valutare performance dividendo dati in multipli fold train-test.",
    "explanation": "K-fold CV divide dati in k parti, addestra su k-1 e testa su 1, ripetendo k volte. Fornisce stime performance robuste.",
    "examples": ["Cross-validation 5-fold", "Leave-one-out CV", "K-fold stratificato"],
    "relatedTerms": ["validation-set", "overfitting", "model-evaluation"]
  },
  {
    "slug": "contrastive-learning",
    "language": "en",
    "term": "Contrastive Learning",
    "category": "Paradigm",
    "definition": "Self-supervised learning contrasting positive pairs against negative pairs.",
    "explanation": "Learns to pull similar items together and push dissimilar items apart. No labels required. Powers CLIP, SimCLR.",
    "examples": ["CLIP vision-language learning", "SimCLR image representations", "Self-supervised pre-training"],
    "relatedTerms": ["self-supervised", "clip", "representation-learning"]
  },
  {
    "slug": "contrastive-learning-it",
    "language": "it",
    "term": "Contrastive Learning",
    "category": "Paradigma",
    "definition": "Apprendimento self-supervised contrastando coppie positive contro coppie negative.",
    "explanation": "Apprende ad avvicinare item simili e allontanare item dissimili. Non servono etichette. Alimenta CLIP, SimCLR.",
    "examples": ["CLIP apprendimento vision-language", "SimCLR rappresentazioni immagini", "Pre-training self-supervised"],
    "relatedTerms": ["self-supervised", "clip", "representation-learning"]
  },
  {
    "slug": "quantization",
    "language": "en",
    "term": "Quantization",
    "category": "Technique",
    "definition": "Reducing precision of weights/activations to lower memory and computation.",
    "explanation": "Converts 32-bit floats to 8-bit integers or lower. Reduces model size 4x with minimal accuracy loss. Essential for edge deployment.",
    "examples": ["INT8 quantization", "Mobile deployment", "4-bit LLM quantization"],
    "relatedTerms": ["model-compression", "edge-ai", "inference"]
  },
  {
    "slug": "quantization-it",
    "language": "it",
    "term": "Quantizzazione",
    "category": "Tecnica",
    "definition": "Ridurre precisione pesi/attivazioni per abbassare memoria e computazione.",
    "explanation": "Converte float 32-bit a interi 8-bit o meno. Riduce dimensione modello 4x con perdita minima accuracy. Essenziale per edge deployment.",
    "examples": ["Quantizzazione INT8", "Deployment mobile", "Quantizzazione 4-bit LLM"],
    "relatedTerms": ["model-compression", "edge-ai", "inference"]
  },
  {
    "slug": "pruning",
    "language": "en",
    "term": "Pruning",
    "category": "Technique",
    "definition": "Removing unnecessary weights/neurons to reduce model size and computational cost.",
    "explanation": "Identifies and removes weights with small magnitudes. Can be structured (neurons/filters) or unstructured (individual weights). Often combined with fine-tuning.",
    "examples": ["Weight pruning by magnitude", "Structured filter pruning", "Lottery ticket hypothesis"],
    "relatedTerms": ["model-compression", "sparsity", "efficiency"]
  },
  {
    "slug": "pruning-it",
    "language": "it",
    "term": "Pruning",
    "category": "Tecnica",
    "definition": "Rimuovere pesi/neuroni non necessari per ridurre dimensione modello e costo computazionale.",
    "explanation": "Identifica e rimuove pesi con magnitudine piccola. Può essere strutturato (neuroni/filtri) o non strutturato (pesi individuali). Spesso combinato con fine-tuning.",
    "examples": ["Pruning pesi per magnitudine", "Pruning strutturato filtri", "Lottery ticket hypothesis"],
    "relatedTerms": ["model-compression", "sparsity", "efficiency"]
  },
  {
    "slug": "lora",
    "language": "en",
    "term": "LoRA (Low-Rank Adaptation)",
    "category": "Technique",
    "definition": "Parameter-efficient fine-tuning adding trainable low-rank matrices to frozen weights.",
    "explanation": "Adds small trainable matrices A and B where ΔW = BA. Reduces trainable parameters to 0.1-1% while maintaining performance. Enables efficient adapter-based fine-tuning.",
    "examples": ["Fine-tuning LLMs with LoRA", "Adapter modules", "Multi-task learning"],
    "relatedTerms": ["fine-tuning", "peft", "adapters"]
  },
  {
    "slug": "lora-it",
    "language": "it",
    "term": "LoRA (Low-Rank Adaptation)",
    "category": "Tecnica",
    "definition": "Fine-tuning parameter-efficient aggiungendo matrici trainabili low-rank a pesi congelati.",
    "explanation": "Aggiunge piccole matrici trainabili A e B dove ΔW = BA. Riduce parametri trainabili a 0.1-1% mantenendo performance. Permette fine-tuning basato su adapter efficiente.",
    "examples": ["Fine-tuning LLM con LoRA", "Moduli adapter", "Multi-task learning"],
    "relatedTerms": ["fine-tuning", "peft", "adapters"]
  },
  {
    "slug": "self-supervised-learning",
    "language": "en",
    "term": "Self-Supervised Learning",
    "category": "Paradigm",
    "definition": "Learning paradigm where models create supervision signal from unlabeled data.",
    "explanation": "Predicts parts of input from other parts (masked LM, image inpainting, next frame). Enables pre-training on massive unlabeled datasets. Foundation of BERT, GPT.",
    "examples": ["BERT masked language modeling", "GPT next token prediction", "Image rotation prediction"],
    "relatedTerms": ["unsupervised-learning", "pre-training", "representation-learning"]
  },
  {
    "slug": "self-supervised-learning-it",
    "language": "it",
    "term": "Self-Supervised Learning",
    "category": "Paradigma",
    "definition": "Paradigma di apprendimento dove i modelli creano segnale di supervisione da dati non etichettati.",
    "explanation": "Predice parti dell'input da altre parti (masked LM, image inpainting, next frame). Permette pre-training su dataset non etichettati massivi. Fondamento di BERT, GPT.",
    "examples": ["BERT masked language modeling", "GPT predizione next token", "Predizione rotazione immagine"],
    "relatedTerms": ["unsupervised-learning", "pre-training", "representation-learning"]
  },
  {
    "slug": "temperature-sampling",
    "language": "en",
    "term": "Temperature (Sampling)",
    "category": "Technique",
    "definition": "Parameter controlling randomness in text generation by scaling logits before softmax.",
    "explanation": "Temperature T divides logits. T < 1 makes distribution sharper (deterministic), T > 1 flatter (random). T = 0 is greedy, T = 1 unmodified.",
    "examples": ["Temperature 0.7 for creative text", "Temperature 0.1 for factual", "Greedy decoding T=0"],
    "relatedTerms": ["text-generation", "sampling", "llm"]
  },
  {
    "slug": "temperature-sampling-it",
    "language": "it",
    "term": "Temperature (Sampling)",
    "category": "Tecnica",
    "definition": "Parametro controllante randomicità in generazione testo scalando logit prima di softmax.",
    "explanation": "Temperature T divide logit. T < 1 rende distribuzione sharp (deterministica), T > 1 flat (random). T = 0 è greedy, T = 1 non modificata.",
    "examples": ["Temperature 0.7 per testo creativo", "Temperature 0.1 per fattuale", "Greedy decoding T=0"],
    "relatedTerms": ["text-generation", "sampling", "llm"]
  },
  {
    "slug": "chain-of-thought",
    "language": "en",
    "term": "Chain-of-Thought Prompting",
    "category": "Technique",
    "definition": "Prompting technique encouraging LLMs to show intermediate reasoning steps before answering.",
    "explanation": "Adding 'Let's think step by step' or reasoning examples improves complex task performance. Enables solving problems impossible with direct prompting.",
    "examples": ["Math word problems", "Logical reasoning", "Multi-step planning"],
    "relatedTerms": ["prompt-engineering", "reasoning", "llm"]
  },
  {
    "slug": "chain-of-thought-it",
    "language": "it",
    "term": "Chain-of-Thought Prompting",
    "category": "Tecnica",
    "definition": "Tecnica di prompting incoraggiante LLM a mostrare step di ragionamento intermedi prima di rispondere.",
    "explanation": "Aggiungere 'Pensiamo passo dopo passo' o esempi di ragionamento migliora performance su task complessi. Permette risolvere problemi impossibili con prompting diretto.",
    "examples": ["Problemi matematici con parole", "Ragionamento logico", "Pianificazione multi-step"],
    "relatedTerms": ["prompt-engineering", "reasoning", "llm"]
  },
  {
    "slug": "mse",
    "language": "en",
    "term": "MSE (Mean Squared Error)",
    "category": "Metric",
    "definition": "Loss function measuring average squared difference between predicted and actual values.",
    "explanation": "MSE = (1/n) * Σ(y_true - y_pred)². Heavily penalizes large errors. Common for regression. Lower is better.",
    "examples": ["Regression loss function", "Neural network training", "Error measurement"],
    "relatedTerms": ["loss-function", "regression", "mae"]
  },
  {
    "slug": "mse-it",
    "language": "it",
    "term": "MSE (Mean Squared Error)",
    "category": "Metrica",
    "definition": "Funzione di loss misurando differenza quadratica media tra valori predetti e reali.",
    "explanation": "MSE = (1/n) * Σ(y_true - y_pred)². Penalizza pesantemente errori grandi. Comune per regressione. Più basso è meglio.",
    "examples": ["Funzione loss regressione", "Training rete neurale", "Misurazione errore"],
    "relatedTerms": ["loss-function", "regression", "mae"]
  },
  {
    "slug": "mae",
    "language": "en",
    "term": "MAE (Mean Absolute Error)",
    "category": "Metric",
    "definition": "Loss function measuring average absolute difference between predicted and actual values.",
    "explanation": "MAE = (1/n) * Σ|y_true - y_pred|. More robust to outliers than MSE. Linear penalty for errors.",
    "examples": ["Robust regression loss", "Outlier-resistant evaluation", "Error magnitude"],
    "relatedTerms": ["loss-function", "regression", "mse"]
  },
  {
    "slug": "mae-it",
    "language": "it",
    "term": "MAE (Mean Absolute Error)",
    "category": "Metrica",
    "definition": "Funzione di loss misurando differenza assoluta media tra valori predetti e reali.",
    "explanation": "MAE = (1/n) * Σ|y_true - y_pred|. Più robusto agli outlier rispetto a MSE. Penalità lineare per errori.",
    "examples": ["Loss regressione robusta", "Valutazione resistente outlier", "Magnitudine errore"],
    "relatedTerms": ["loss-function", "regression", "mse"]
  },
  {
    "slug": "perplexity",
    "language": "en",
    "term": "Perplexity",
    "category": "Metric",
    "definition": "Measurement of how well a probability model predicts a sample, for evaluating language models.",
    "explanation": "Perplexity = exp(cross_entropy). Lower perplexity indicates better prediction. Commonly used for LM evaluation.",
    "examples": ["Language model evaluation", "GPT performance metric", "Text generation quality"],
    "relatedTerms": ["language-model", "cross-entropy", "nlp"]
  },
  {
    "slug": "perplexity-it",
    "language": "it",
    "term": "Perplexity",
    "category": "Metrica",
    "definition": "Misurazione di quanto bene un modello probabilistico predice un campione, per valutare language model.",
    "explanation": "Perplexity = exp(cross_entropy). Perplexity più bassa indica predizione migliore. Comunemente usata per valutazione LM.",
    "examples": ["Valutazione language model", "Metrica performance GPT", "Qualità generazione testo"],
    "relatedTerms": ["language-model", "cross-entropy", "nlp"]
  },
  {
    "slug": "bleu-score",
    "language": "en",
    "term": "BLEU Score",
    "category": "Metric",
    "definition": "Metric for machine translation quality comparing n-gram overlap between generated and reference translations.",
    "explanation": "Measures precision of n-grams (1-4) with brevity penalty. Score ranges 0-1 or 0-100. Higher is better.",
    "examples": ["Machine translation evaluation", "Text generation quality", "MT model comparison"],
    "relatedTerms": ["machine-translation", "nlp", "evaluation-metric"]
  },
  {
    "slug": "bleu-score-it",
    "language": "it",
    "term": "BLEU Score",
    "category": "Metrica",
    "definition": "Metrica per qualità traduzione automatica comparando sovrapposizione n-gram tra traduzioni generate e riferimento.",
    "explanation": "Misura precision n-gram (1-4) con penalità brevità. Score varia 0-1 o 0-100. Più alto è meglio.",
    "examples": ["Valutazione traduzione automatica", "Qualità generazione testo", "Comparazione modelli MT"],
    "relatedTerms": ["machine-translation", "nlp", "evaluation-metric"]
  },
  {
    "slug": "accuracy",
    "language": "en",
    "term": "Accuracy",
    "category": "Metric",
    "definition": "Proportion of correct predictions out of total predictions made.",
    "explanation": "Accuracy = (TP + TN) / (TP + TN + FP + FN). Simple metric but misleading for imbalanced datasets.",
    "examples": ["Overall classification performance", "Balanced dataset evaluation", "Model comparison"],
    "relatedTerms": ["classification", "precision-recall", "confusion-matrix"]
  },
  {
    "slug": "accuracy-it",
    "language": "it",
    "term": "Accuracy",
    "category": "Metrica",
    "definition": "Proporzione di predizioni corrette sul totale delle predizioni fatte.",
    "explanation": "Accuracy = (TP + TN) / (TP + TN + FP + FN). Metrica semplice ma fuorviante per dataset sbilanciati.",
    "examples": ["Performance classificazione globale", "Valutazione dataset bilanciato", "Comparazione modelli"],
    "relatedTerms": ["classification", "precision-recall", "confusion-matrix"]
  },
  {
    "slug": "top-k-sampling",
    "language": "en",
    "term": "Top-K Sampling",
    "category": "Technique",
    "definition": "Text generation technique sampling from only the K most likely next tokens.",
    "explanation": "Restricts sampling to top K tokens by probability. Reduces unlikely tokens while maintaining diversity. Common: K=40-50.",
    "examples": ["K=50 for balanced generation", "Diverse text generation", "Avoiding rare tokens"],
    "relatedTerms": ["text-generation", "nucleus-sampling", "llm"]
  },
  {
    "slug": "top-k-sampling-it",
    "language": "it",
    "term": "Top-K Sampling",
    "category": "Tecnica",
    "definition": "Tecnica generazione testo campionando solo dai K token successivi più probabili.",
    "explanation": "Restringe sampling ai top K token per probabilità. Riduce token improbabili mantenendo diversità. Comune: K=40-50.",
    "examples": ["K=50 per generazione bilanciata", "Generazione testo diversa", "Evitare token rari"],
    "relatedTerms": ["text-generation", "nucleus-sampling", "llm"]
  },
  {
    "slug": "nucleus-sampling",
    "language": "en",
    "term": "Nucleus Sampling (Top-P)",
    "category": "Technique",
    "definition": "Text generation sampling from smallest set of tokens whose cumulative probability exceeds threshold P.",
    "explanation": "Dynamically adjusts vocabulary size based on probability distribution. More flexible than top-k. Common: P=0.9-0.95.",
    "examples": ["P=0.9 for diverse generation", "Dynamic vocabulary selection", "Better than top-k"],
    "relatedTerms": ["text-generation", "top-k-sampling", "llm"]
  },
  {
    "slug": "nucleus-sampling-it",
    "language": "it",
    "term": "Nucleus Sampling (Top-P)",
    "category": "Tecnica",
    "definition": "Generazione testo campionando dal set più piccolo di token la cui probabilità cumulativa supera soglia P.",
    "explanation": "Aggiusta dinamicamente dimensione vocabolario basandosi su distribuzione probabilità. Più flessibile di top-k. Comune: P=0.9-0.95.",
    "examples": ["P=0.9 per generazione diversa", "Selezione vocabolario dinamica", "Migliore di top-k"],
    "relatedTerms": ["text-generation", "top-k-sampling", "llm"]
  },
  {
    "slug": "beam-search",
    "language": "en",
    "term": "Beam Search",
    "category": "Algorithm",
    "definition": "Decoding algorithm keeping top B most likely sequences at each step.",
    "explanation": "Explores multiple hypotheses simultaneously. Beam width B controls quality-speed tradeoff. Produces more coherent but less diverse outputs.",
    "examples": ["Beam width 5 for translation", "Deterministic decoding", "Machine translation"],
    "relatedTerms": ["text-generation", "decoding", "sequence-to-sequence"]
  },
  {
    "slug": "beam-search-it",
    "language": "it",
    "term": "Beam Search",
    "category": "Algoritmo",
    "definition": "Algoritmo di decoding mantenendo top B sequenze più probabili a ogni step.",
    "explanation": "Esplora multiple ipotesi simultaneamente. Beam width B controlla tradeoff qualità-velocità. Produce output più coerenti ma meno diversi.",
    "examples": ["Beam width 5 per traduzione", "Decoding deterministico", "Traduzione automatica"],
    "relatedTerms": ["text-generation", "decoding", "sequence-to-sequence"]
  },
  {
    "slug": "gru",
    "language": "en",
    "term": "GRU (Gated Recurrent Unit)",
    "category": "Architecture",
    "definition": "Simplified RNN variant with gating mechanisms, similar to LSTM but fewer parameters.",
    "explanation": "Has reset and update gates (vs LSTM's 3 gates). Often performs similarly to LSTM with faster computation. Good for sequential data.",
    "examples": ["Time series modeling", "Text generation", "Sequence prediction"],
    "relatedTerms": ["lstm", "rnn", "sequence-modeling"]
  },
  {
    "slug": "gru-it",
    "language": "it",
    "term": "GRU (Gated Recurrent Unit)",
    "category": "Architettura",
    "definition": "Variante RNN semplificata con meccanismi gating, simile a LSTM ma meno parametri.",
    "explanation": "Ha gate reset e update (vs 3 gate di LSTM). Spesso performance simili a LSTM con computazione più veloce. Buono per dati sequenziali.",
    "examples": ["Modellazione serie temporali", "Generazione testo", "Predizione sequenze"],
    "relatedTerms": ["lstm", "rnn", "sequence-modeling"]
  },
  {
    "slug": "attention-score",
    "language": "en",
    "term": "Attention Score",
    "category": "Concept",
    "definition": "Computed similarity between query and key vectors before softmax normalization in attention.",
    "explanation": "Score = dot_product(Query, Key) / sqrt(d_k). Scaled dot-product prevents gradient issues. Higher scores mean more relevance.",
    "examples": ["Scaled dot-product attention", "Query-key similarity", "Attention computation"],
    "relatedTerms": ["attention-mechanism", "query-key-value", "softmax"]
  },
  {
    "slug": "attention-score-it",
    "language": "it",
    "term": "Attention Score",
    "category": "Concetto",
    "definition": "Similarità calcolata tra vettori query e key prima della normalizzazione softmax nell'attention.",
    "explanation": "Score = dot_product(Query, Key) / sqrt(d_k). Scaled dot-product previene problemi di gradiente. Score più alti significano più rilevanza.",
    "examples": ["Scaled dot-product attention", "Similarità query-key", "Computazione attention"],
    "relatedTerms": ["attention-mechanism", "query-key-value", "softmax"]
  },
  {
    "slug": "causal-masking",
    "language": "en",
    "term": "Causal Masking",
    "category": "Technique",
    "definition": "Masking technique preventing attention to future positions in autoregressive models.",
    "explanation": "Ensures token at position i can only attend to positions ≤ i. Essential for GPT-style models. Implemented as triangular mask.",
    "examples": ["GPT autoregressive generation", "Decoder self-attention", "Preventing future leakage"],
    "relatedTerms": ["autoregressive", "transformer", "gpt"]
  },
  {
    "slug": "causal-masking-it",
    "language": "it",
    "term": "Causal Masking",
    "category": "Tecnica",
    "definition": "Tecnica di masking prevenendo attention a posizioni future in modelli autoregressivi.",
    "explanation": "Assicura che token a posizione i può attendere solo posizioni ≤ i. Essenziale per modelli stile GPT. Implementato come mask triangolare.",
    "examples": ["GPT generazione autoregressive", "Decoder self-attention", "Prevenire future leakage"],
    "relatedTerms": ["autoregressive", "transformer", "gpt"]
  },
  {
    "slug": "masked-language-modeling",
    "language": "en",
    "term": "Masked Language Modeling",
    "category": "Task",
    "definition": "Pre-training task where random tokens are masked and model predicts them from context.",
    "explanation": "15% of tokens masked: 80% [MASK], 10% random, 10% unchanged. BERT's pre-training objective. Enables bidirectional context.",
    "examples": ["BERT pre-training", "Cloze task", "Bidirectional language understanding"],
    "relatedTerms": ["bert", "pre-training", "self-supervised-learning"]
  },
  {
    "slug": "masked-language-modeling-it",
    "language": "it",
    "term": "Masked Language Modeling",
    "category": "Task",
    "definition": "Task di pre-training dove token random sono masked e il modello li predice dal contesto.",
    "explanation": "15% token masked: 80% [MASK], 10% random, 10% invariati. Obiettivo pre-training di BERT. Permette contesto bidirezionale.",
    "examples": ["Pre-training BERT", "Cloze task", "Comprensione linguaggio bidirezionale"],
    "relatedTerms": ["bert", "pre-training", "self-supervised-learning"]
  },
  {
    "slug": "next-sentence-prediction",
    "language": "en",
    "term": "Next Sentence Prediction",
    "category": "Task",
    "definition": "Pre-training task predicting whether sentence B follows sentence A.",
    "explanation": "Binary classification: 50% consecutive, 50% random pairs. Originally used in BERT but later found less useful than MLM alone.",
    "examples": ["BERT pre-training task", "Sentence relationship", "Discourse understanding"],
    "relatedTerms": ["bert", "pre-training", "sentence-embeddings"]
  },
  {
    "slug": "next-sentence-prediction-it",
    "language": "it",
    "term": "Next Sentence Prediction",
    "category": "Task",
    "definition": "Task di pre-training predicendo se frase B segue frase A.",
    "explanation": "Classificazione binaria: 50% consecutive, 50% coppie random. Originalmente usato in BERT ma poi trovato meno utile di MLM solo.",
    "examples": ["Task pre-training BERT", "Relazione frasi", "Comprensione discorso"],
    "relatedTerms": ["bert", "pre-training", "sentence-embeddings"]
  },
  {
    "slug": "named-entity-recognition",
    "language": "en",
    "term": "NER (Named Entity Recognition)",
    "category": "Task",
    "definition": "NLP task identifying and classifying named entities (persons, organizations, locations) in text.",
    "explanation": "Sequence labeling task using BIO tagging (Begin, Inside, Outside). Common with BERT-based models. Essential for information extraction.",
    "examples": ["Extract person names", "Identify organizations", "Location detection"],
    "relatedTerms": ["nlp", "sequence-labeling", "information-extraction"]
  },
  {
    "slug": "named-entity-recognition-it",
    "language": "it",
    "term": "NER (Named Entity Recognition)",
    "category": "Task",
    "definition": "Task NLP identificando e classificando entità nominate (persone, organizzazioni, luoghi) nel testo.",
    "explanation": "Task sequence labeling usando BIO tagging (Begin, Inside, Outside). Comune con modelli basati su BERT. Essenziale per information extraction.",
    "examples": ["Estrarre nomi persone", "Identificare organizzazioni", "Rilevamento luoghi"],
    "relatedTerms": ["nlp", "sequence-labeling", "information-extraction"]
  },
  {
    "slug": "sentiment-analysis",
    "language": "en",
    "term": "Sentiment Analysis",
    "category": "Task",
    "definition": "NLP task determining emotional tone or opinion expressed in text.",
    "explanation": "Classification into positive/negative/neutral or finer emotions. Used for review analysis, social media monitoring. Common with fine-tuned BERT.",
    "examples": ["Product review analysis", "Social media sentiment", "Customer feedback"],
    "relatedTerms": ["nlp", "classification", "text-classification"]
  },
  {
    "slug": "sentiment-analysis-it",
    "language": "it",
    "term": "Sentiment Analysis",
    "category": "Task",
    "definition": "Task NLP determinando tono emotivo o opinione espressa nel testo.",
    "explanation": "Classificazione in positivo/negativo/neutrale o emozioni più fini. Usata per analisi recensioni, monitoraggio social media. Comune con BERT fine-tuned.",
    "examples": ["Analisi recensioni prodotti", "Sentiment social media", "Feedback clienti"],
    "relatedTerms": ["nlp", "classification", "text-classification"]
  },
  {
    "slug": "question-answering",
    "language": "en",
    "term": "Question Answering",
    "category": "Task",
    "definition": "NLP task where model extracts or generates answers to questions based on context.",
    "explanation": "Extractive QA (SQuAD) selects span from context. Generative QA produces free-form answers. Powered by BERT, GPT for different variants.",
    "examples": ["SQuAD dataset", "Reading comprehension", "Open-domain QA"],
    "relatedTerms": ["nlp", "bert", "extractive-qa"]
  },
  {
    "slug": "question-answering-it",
    "language": "it",
    "term": "Question Answering",
    "category": "Task",
    "definition": "Task NLP dove il modello estrae o genera risposte a domande basandosi sul contesto.",
    "explanation": "QA estrattivo (SQuAD) seleziona span dal contesto. QA generativo produce risposte libere. Alimentato da BERT, GPT per varianti diverse.",
    "examples": ["Dataset SQuAD", "Reading comprehension", "QA open-domain"],
    "relatedTerms": ["nlp", "bert", "extractive-qa"]
  },
  {
    "slug": "text-summarization",
    "language": "en",
    "term": "Text Summarization",
    "category": "Task",
    "definition": "NLP task condensing long text while preserving key information.",
    "explanation": "Extractive summarization selects important sentences. Abstractive summarization generates new text. T5, BART excel at abstractive.",
    "examples": ["News article summarization", "Document condensation", "Abstract generation"],
    "relatedTerms": ["nlp", "seq2seq", "abstractive-summarization"]
  },
  {
    "slug": "text-summarization-it",
    "language": "it",
    "term": "Text Summarization",
    "category": "Task",
    "definition": "Task NLP condensando testo lungo preservando informazioni chiave.",
    "explanation": "Summarization estrattiva seleziona frasi importanti. Summarization astrattiva genera nuovo testo. T5, BART eccellono in astrattiva.",
    "examples": ["Summarization articoli news", "Condensazione documenti", "Generazione abstract"],
    "relatedTerms": ["nlp", "seq2seq", "abstractive-summarization"]
  },
  {
    "slug": "machine-translation",
    "language": "en",
    "term": "Machine Translation",
    "category": "Task",
    "definition": "Automatic translation of text from one language to another.",
    "explanation": "Sequence-to-sequence task. Neural MT (NMT) uses encoder-decoder with attention. Transformers (like mT5, mBART) are state-of-the-art.",
    "examples": ["Google Translate", "English to French", "Multilingual models"],
    "relatedTerms": ["seq2seq", "transformer", "encoder-decoder"]
  },
  {
    "slug": "machine-translation-it",
    "language": "it",
    "term": "Machine Translation",
    "category": "Task",
    "definition": "Traduzione automatica di testo da una lingua all'altra.",
    "explanation": "Task sequence-to-sequence. Neural MT (NMT) usa encoder-decoder con attention. Transformer (come mT5, mBART) sono state-of-the-art.",
    "examples": ["Google Translate", "Inglese a Francese", "Modelli multilingua"],
    "relatedTerms": ["seq2seq", "transformer", "encoder-decoder"]
  },
  {
    "slug": "inception-module",
    "language": "en",
    "term": "Inception Module",
    "category": "Architecture",
    "definition": "CNN building block applying multiple filter sizes in parallel and concatenating results.",
    "explanation": "Captures features at different scales simultaneously. Uses 1x1, 3x3, 5x5 convolutions plus pooling. Reduces parameters with 1x1 bottleneck convolutions.",
    "examples": ["GoogLeNet/Inception v1", "Multi-scale feature extraction", "Efficient CNN design"],
    "relatedTerms": ["cnn", "googlenet", "multi-scale"]
  },
  {
    "slug": "inception-module-it",
    "language": "it",
    "term": "Inception Module",
    "category": "Architettura",
    "definition": "Blocco CNN applicando dimensioni filtro multiple in parallelo e concatenando risultati.",
    "explanation": "Cattura feature a scale diverse simultaneamente. Usa convoluzioni 1x1, 3x3, 5x5 più pooling. Riduce parametri con convoluzioni bottleneck 1x1.",
    "examples": ["GoogLeNet/Inception v1", "Estrazione feature multi-scala", "Design CNN efficiente"],
    "relatedTerms": ["cnn", "googlenet", "multi-scale"]
  },
  {
    "slug": "vit",
    "language": "en",
    "term": "ViT (Vision Transformer)",
    "category": "Architecture",
    "definition": "Transformer architecture adapted for computer vision by treating image patches as tokens.",
    "explanation": "Divides images into patches (16x16), flattens and projects them, adds positional encoding. Pure transformer without convolutions. Requires large datasets.",
    "examples": ["Image classification", "Alternative to CNNs", "ViT-Base, ViT-Large"],
    "relatedTerms": ["transformer", "computer-vision", "patch-embedding"]
  },
  {
    "slug": "vit-it",
    "language": "it",
    "term": "ViT (Vision Transformer)",
    "category": "Architettura",
    "definition": "Architettura Transformer adattata per computer vision trattando patch immagini come token.",
    "explanation": "Divide immagini in patch (16x16), le appiattisce e proietta, aggiunge positional encoding. Transformer puro senza convoluzioni. Richiede dataset grandi.",
    "examples": ["Classificazione immagini", "Alternativa a CNN", "ViT-Base, ViT-Large"],
    "relatedTerms": ["transformer", "computer-vision", "patch-embedding"]
  },
  {
    "slug": "alexnet",
    "language": "en",
    "term": "AlexNet",
    "category": "Architecture",
    "definition": "Deep CNN that won ImageNet 2012, pioneering deep learning in computer vision.",
    "explanation": "8 layers (5 conv, 3 FC), ReLU activation, dropout, data augmentation. First to use GPU training at scale. Sparked deep learning revolution.",
    "examples": ["ImageNet 2012 winner", "Deep learning breakthrough", "GPU-accelerated training"],
    "relatedTerms": ["cnn", "imagenet", "deep-learning"]
  },
  {
    "slug": "alexnet-it",
    "language": "it",
    "term": "AlexNet",
    "category": "Architettura",
    "definition": "CNN profonda vincitrice ImageNet 2012, pioniera del deep learning in computer vision.",
    "explanation": "8 layer (5 conv, 3 FC), attivazione ReLU, dropout, data augmentation. Prima a usare GPU training su larga scala. Ha innescato rivoluzione deep learning.",
    "examples": ["Vincitore ImageNet 2012", "Breakthrough deep learning", "Training accelerato GPU"],
    "relatedTerms": ["cnn", "imagenet", "deep-learning"]
  },
  {
    "slug": "vgg",
    "language": "en",
    "term": "VGG",
    "category": "Architecture",
    "definition": "Deep CNN architecture using small 3x3 filters throughout, emphasizing depth.",
    "explanation": "VGG-16 and VGG-19 with 16/19 layers. Simple, uniform architecture. Multiple 3x3 convs approximate larger receptive fields efficiently.",
    "examples": ["VGG-16 for transfer learning", "Feature extraction backbone", "ImageNet classification"],
    "relatedTerms": ["cnn", "deep-learning", "imagenet"]
  },
  {
    "slug": "vgg-it",
    "language": "it",
    "term": "VGG",
    "category": "Architettura",
    "definition": "Architettura CNN profonda usando filtri piccoli 3x3 ovunque, enfatizzando profondità.",
    "explanation": "VGG-16 e VGG-19 con 16/19 layer. Architettura semplice e uniforme. Più conv 3x3 approssimano campi ricettivi più grandi efficientemente.",
    "examples": ["VGG-16 per transfer learning", "Backbone estrazione feature", "Classificazione ImageNet"],
    "relatedTerms": ["cnn", "deep-learning", "imagenet"]
  },
  {
    "slug": "mobilenet",
    "language": "en",
    "term": "MobileNet",
    "category": "Architecture",
    "definition": "Efficient CNN architecture using depthwise separable convolutions for mobile deployment.",
    "explanation": "Separates spatial and channel-wise convolutions. Drastically reduces parameters and computation. Width multiplier controls model size.",
    "examples": ["Mobile applications", "Edge AI", "Real-time inference"],
    "relatedTerms": ["efficient-architecture", "edge-ai", "depthwise-convolution"]
  },
  {
    "slug": "mobilenet-it",
    "language": "it",
    "term": "MobileNet",
    "category": "Architettura",
    "definition": "Architettura CNN efficiente usando convoluzioni depthwise separabili per deployment mobile.",
    "explanation": "Separa convoluzioni spaziali e channel-wise. Riduce drasticamente parametri e computazione. Width multiplier controlla dimensione modello.",
    "examples": ["Applicazioni mobile", "Edge AI", "Inference real-time"],
    "relatedTerms": ["efficient-architecture", "edge-ai", "depthwise-convolution"]
  },
  {
    "slug": "rmsprop",
    "language": "en",
    "term": "RMSprop",
    "category": "Algorithm",
    "definition": "Adaptive learning rate optimization algorithm using moving average of squared gradients.",
    "explanation": "Divides learning rate by exponential moving average of squared gradients. Addresses Adagrad's diminishing learning rates. Good for RNNs and non-stationary problems.",
    "examples": ["RNN training", "Non-convex optimization", "Adaptive learning"],
    "relatedTerms": ["optimizer", "adam", "gradient-descent"]
  },
  {
    "slug": "rmsprop-it",
    "language": "it",
    "term": "RMSprop",
    "category": "Algoritmo",
    "definition": "Algoritmo di ottimizzazione con learning rate adattivo usando media mobile di gradienti al quadrato.",
    "explanation": "Divide learning rate per media mobile esponenziale di gradienti al quadrato. Affronta i learning rate diminuenti di Adagrad. Buono per RNN e problemi non-stazionari.",
    "examples": ["Training RNN", "Ottimizzazione non-convessa", "Apprendimento adattivo"],
    "relatedTerms": ["optimizer", "adam", "gradient-descent"]
  },
  {
    "slug": "adagrad",
    "language": "en",
    "term": "Adagrad",
    "category": "Algorithm",
    "definition": "Adaptive learning rate optimizer that adapts rates per parameter based on historical gradients.",
    "explanation": "Accumulates squared gradients, larger accumulation means smaller learning rate. Good for sparse data. Can have diminishing learning rates over time.",
    "examples": ["Sparse gradient optimization", "NLP embeddings", "Convex problems"],
    "relatedTerms": ["optimizer", "rmsprop", "adaptive-learning"]
  },
  {
    "slug": "adagrad-it",
    "language": "it",
    "term": "Adagrad",
    "category": "Algoritmo",
    "definition": "Ottimizzatore con learning rate adattivo che adatta rate per parametro basandosi su gradienti storici.",
    "explanation": "Accumula gradienti al quadrato, accumulazione più grande significa learning rate più piccolo. Buono per dati sparsi. Può avere learning rate diminuenti nel tempo.",
    "examples": ["Ottimizzazione gradienti sparsi", "Embedding NLP", "Problemi convessi"],
    "relatedTerms": ["optimizer", "rmsprop", "adaptive-learning"]
  },
  {
    "slug": "imagenet",
    "language": "en",
    "term": "ImageNet",
    "category": "Dataset",
    "definition": "Large-scale image dataset with 14M images across 20k categories, used for ILSVRC competition.",
    "explanation": "ILSVRC uses 1000-class subset. Benchmark for computer vision models. Pre-training on ImageNet enables transfer learning for vision tasks.",
    "examples": ["ImageNet classification challenge", "Pre-training for transfer learning", "Benchmark dataset"],
    "relatedTerms": ["computer-vision", "transfer-learning", "benchmark"]
  },
  {
    "slug": "imagenet-it",
    "language": "it",
    "term": "ImageNet",
    "category": "Dataset",
    "definition": "Dataset immagini large-scale con 14M immagini attraverso 20k categorie, usato per competizione ILSVRC.",
    "explanation": "ILSVRC usa subset 1000 classi. Benchmark per modelli computer vision. Pre-training su ImageNet permette transfer learning per task vision.",
    "examples": ["Challenge classificazione ImageNet", "Pre-training per transfer learning", "Dataset benchmark"],
    "relatedTerms": ["computer-vision", "transfer-learning", "benchmark"]
  },
  {
    "slug": "coco-dataset",
    "language": "en",
    "term": "COCO (Common Objects in Context)",
    "category": "Dataset",
    "definition": "Large-scale dataset for object detection, segmentation, and captioning with 330k images.",
    "explanation": "Contains 80 object categories with instance segmentation masks. Includes captions, keypoints. Standard benchmark for detection and segmentation.",
    "examples": ["Object detection benchmark", "Instance segmentation", "Image captioning"],
    "relatedTerms": ["object-detection", "segmentation", "benchmark"]
  },
  {
    "slug": "coco-dataset-it",
    "language": "it",
    "term": "COCO (Common Objects in Context)",
    "category": "Dataset",
    "definition": "Dataset large-scale per object detection, segmentazione e captioning con 330k immagini.",
    "explanation": "Contiene 80 categorie oggetti con mask segmentazione instance. Include caption, keypoint. Benchmark standard per detection e segmentazione.",
    "examples": ["Benchmark object detection", "Segmentazione instance", "Image captioning"],
    "relatedTerms": ["object-detection", "segmentation", "benchmark"]
  },
  {
    "slug": "squad",
    "language": "en",
    "term": "SQuAD (Stanford Question Answering Dataset)",
    "category": "Dataset",
    "definition": "Reading comprehension dataset with 100k+ questions on Wikipedia articles.",
    "explanation": "Extractive QA: answers are spans from context. SQuAD 2.0 includes unanswerable questions. Benchmark for question answering models.",
    "examples": ["BERT fine-tuning for QA", "Extractive question answering", "Reading comprehension"],
    "relatedTerms": ["question-answering", "nlp", "benchmark"]
  },
  {
    "slug": "squad-it",
    "language": "it",
    "term": "SQuAD (Stanford Question Answering Dataset)",
    "category": "Dataset",
    "definition": "Dataset reading comprehension con 100k+ domande su articoli Wikipedia.",
    "explanation": "QA estrattivo: risposte sono span dal contesto. SQuAD 2.0 include domande senza risposta. Benchmark per modelli question answering.",
    "examples": ["Fine-tuning BERT per QA", "Question answering estrattivo", "Reading comprehension"],
    "relatedTerms": ["question-answering", "nlp", "benchmark"]
  },
  {
    "slug": "batch-normalization-en",
    "language": "en",
    "term": "Batch Normalization",
    "category": "Technique",
    "definition": "Normalizes layer inputs using batch statistics to stabilize and accelerate training.",
    "explanation": "Normalizes using batch mean/variance, then scales/shifts with learnable parameters. Reduces internal covariate shift. Allows higher learning rates.",
    "examples": ["After conv layers", "Before activation", "Stabilizing deep networks"],
    "relatedTerms": ["layer-normalization", "normalization", "training"]
  },
  {
    "slug": "instance-normalization",
    "language": "en",
    "term": "Instance Normalization",
    "category": "Technique",
    "definition": "Normalizes each sample independently, commonly used in style transfer.",
    "explanation": "Normalizes across spatial dimensions per instance. Removes style information while preserving content. Popular in GANs and style transfer.",
    "examples": ["Style transfer networks", "GAN generators", "Real-time image stylization"],
    "relatedTerms": ["batch-normalization", "style-transfer", "normalization"]
  },
  {
    "slug": "instance-normalization-it",
    "language": "it",
    "term": "Instance Normalization",
    "category": "Tecnica",
    "definition": "Normalizza ogni campione indipendentemente, comunemente usata in style transfer.",
    "explanation": "Normalizza attraverso dimensioni spaziali per instance. Rimuove informazioni di stile preservando contenuto. Popolare in GAN e style transfer.",
    "examples": ["Reti style transfer", "Generator GAN", "Stylization immagini real-time"],
    "relatedTerms": ["batch-normalization", "style-transfer", "normalization"]
  },
  {
    "slug": "group-normalization",
    "language": "en",
    "term": "Group Normalization",
    "category": "Technique",
    "definition": "Normalizes by dividing channels into groups and normalizing within each group.",
    "explanation": "Independent of batch size, unlike batch norm. Divides channels into groups. Good for small batches and video tasks.",
    "examples": ["Small batch training", "Video understanding", "Object detection"],
    "relatedTerms": ["batch-normalization", "normalization", "small-batch"]
  },
  {
    "slug": "group-normalization-it",
    "language": "it",
    "term": "Group Normalization",
    "category": "Tecnica",
    "definition": "Normalizza dividendo canali in gruppi e normalizzando all'interno di ogni gruppo.",
    "explanation": "Indipendente da batch size, a differenza di batch norm. Divide canali in gruppi. Buona per batch piccoli e task video.",
    "examples": ["Training batch piccoli", "Comprensione video", "Object detection"],
    "relatedTerms": ["batch-normalization", "normalization", "small-batch"]
  },
  {
    "slug": "dilated-convolution",
    "language": "en",
    "term": "Dilated Convolution",
    "category": "Technique",
    "definition": "Convolution with gaps between kernel elements, increasing receptive field without adding parameters.",
    "explanation": "Dilation rate controls spacing. Expands receptive field exponentially with depth. Used in semantic segmentation and audio generation.",
    "examples": ["DeepLab segmentation", "WaveNet audio", "Large receptive fields"],
    "relatedTerms": ["convolution", "receptive-field", "segmentation"]
  },
  {
    "slug": "dilated-convolution-it",
    "language": "it",
    "term": "Dilated Convolution",
    "category": "Tecnica",
    "definition": "Convoluzione con gap tra elementi kernel, aumentando receptive field senza aggiungere parametri.",
    "explanation": "Dilation rate controlla spaziatura. Espande receptive field esponenzialmente con profondità. Usata in segmentazione semantica e generazione audio.",
    "examples": ["Segmentazione DeepLab", "Audio WaveNet", "Receptive field grandi"],
    "relatedTerms": ["convolution", "receptive-field", "segmentation"]
  },
  {
    "slug": "depthwise-separable-convolution",
    "language": "en",
    "term": "Depthwise Separable Convolution",
    "category": "Technique",
    "definition": "Factorizes standard convolution into depthwise and pointwise convolutions for efficiency.",
    "explanation": "Depthwise convolves each channel separately. Pointwise (1x1) combines channels. Drastically reduces computation. Core of MobileNet.",
    "examples": ["MobileNet architecture", "Efficient CNNs", "Mobile deployment"],
    "relatedTerms": ["mobilenet", "efficient-architecture", "convolution"]
  },
  {
    "slug": "depthwise-separable-convolution-it",
    "language": "it",
    "term": "Depthwise Separable Convolution",
    "category": "Tecnica",
    "definition": "Fattorizza convoluzione standard in convoluzioni depthwise e pointwise per efficienza.",
    "explanation": "Depthwise convolve ogni canale separatamente. Pointwise (1x1) combina canali. Riduce drasticamente computazione. Core di MobileNet.",
    "examples": ["Architettura MobileNet", "CNN efficienti", "Deployment mobile"],
    "relatedTerms": ["mobilenet", "efficient-architecture", "convolution"]
  },
  {
    "slug": "focal-loss",
    "language": "en",
    "term": "Focal Loss",
    "category": "Technique",
    "definition": "Loss function addressing class imbalance by down-weighting easy examples.",
    "explanation": "Adds modulating factor (1-p)^γ to cross-entropy. Focuses training on hard examples. Used in object detection (RetinaNet) for handling background class imbalance.",
    "examples": ["RetinaNet object detection", "Imbalanced classification", "Hard example mining"],
    "relatedTerms": ["loss-function", "class-imbalance", "object-detection"]
  },
  {
    "slug": "focal-loss-it",
    "language": "it",
    "term": "Focal Loss",
    "category": "Tecnica",
    "definition": "Funzione di loss affrontando sbilanciamento classi down-weighting esempi facili.",
    "explanation": "Aggiunge fattore modulante (1-p)^γ a cross-entropy. Focalizza training su esempi difficili. Usata in object detection (RetinaNet) per gestire sbilanciamento classe background.",
    "examples": ["Object detection RetinaNet", "Classificazione sbilanciata", "Hard example mining"],
    "relatedTerms": ["loss-function", "class-imbalance", "object-detection"]
  },
  {
    "slug": "dice-loss",
    "language": "en",
    "term": "Dice Loss",
    "category": "Technique",
    "definition": "Loss function based on Dice coefficient, commonly used for segmentation tasks.",
    "explanation": "Measures overlap between predicted and ground truth masks. Range 0-1. Handles class imbalance better than cross-entropy for segmentation.",
    "examples": ["Medical image segmentation", "Semantic segmentation", "Binary masks"],
    "relatedTerms": ["segmentation", "loss-function", "iou"]
  },
  {
    "slug": "dice-loss-it",
    "language": "it",
    "term": "Dice Loss",
    "category": "Tecnica",
    "definition": "Funzione di loss basata su coefficiente Dice, comunemente usata per task di segmentazione.",
    "explanation": "Misura sovrapposizione tra mask predette e ground truth. Range 0-1. Gestisce sbilanciamento classi meglio di cross-entropy per segmentazione.",
    "examples": ["Segmentazione immagini mediche", "Segmentazione semantica", "Mask binarie"],
    "relatedTerms": ["segmentation", "loss-function", "iou"]
  },
  {
    "slug": "triplet-loss",
    "language": "en",
    "term": "Triplet Loss",
    "category": "Technique",
    "definition": "Loss function learning embeddings by minimizing distance between anchor-positive and maximizing anchor-negative.",
    "explanation": "Uses triplets: anchor, positive (same class), negative (different class). Learns discriminative embeddings. Used in face recognition and metric learning.",
    "examples": ["Face recognition (FaceNet)", "Image retrieval", "Metric learning"],
    "relatedTerms": ["metric-learning", "embedding", "contrastive-learning"]
  },
  {
    "slug": "triplet-loss-it",
    "language": "it",
    "term": "Triplet Loss",
    "category": "Tecnica",
    "definition": "Funzione di loss apprendendo embedding minimizzando distanza anchor-positive e massimizzando anchor-negative.",
    "explanation": "Usa triple: anchor, positive (stessa classe), negative (classe diversa). Apprende embedding discriminativi. Usata in face recognition e metric learning.",
    "examples": ["Face recognition (FaceNet)", "Image retrieval", "Metric learning"],
    "relatedTerms": ["metric-learning", "embedding", "contrastive-learning"]
  },
  {
    "slug": "adversarial-training",
    "language": "en",
    "term": "Adversarial Training",
    "category": "Technique",
    "definition": "Training technique improving model robustness by including adversarial examples.",
    "explanation": "Augments training with adversarially perturbed examples. Models learn to be robust to small input perturbations. Improves security against adversarial attacks.",
    "examples": ["Defending against adversarial attacks", "Robust image classification", "Security enhancement"],
    "relatedTerms": ["adversarial-examples", "robustness", "security"]
  },
  {
    "slug": "adversarial-training-it",
    "language": "it",
    "term": "Adversarial Training",
    "category": "Tecnica",
    "definition": "Tecnica di training migliorando robustezza modello includendo esempi adversarial.",
    "explanation": "Aumenta training con esempi perturbati adversarially. I modelli apprendono ad essere robusti a piccole perturbazioni input. Migliora sicurezza contro attacchi adversarial.",
    "examples": ["Difesa contro attacchi adversarial", "Classificazione immagini robusta", "Miglioramento sicurezza"],
    "relatedTerms": ["adversarial-examples", "robustness", "security"]
  },
  {
    "slug": "mixup",
    "language": "en",
    "term": "Mixup",
    "category": "Technique",
    "definition": "Data augmentation creating synthetic examples by mixing pairs of training samples.",
    "explanation": "Linearly interpolates between two samples and their labels: x = λx₁ + (1-λ)x₂. Improves generalization and calibration. Simple yet effective regularization.",
    "examples": ["Image classification augmentation", "Regularization technique", "Improved generalization"],
    "relatedTerms": ["data-augmentation", "regularization", "cutmix"]
  },
  {
    "slug": "mixup-it",
    "language": "it",
    "term": "Mixup",
    "category": "Tecnica",
    "definition": "Data augmentation creando esempi sintetici mixando coppie di campioni training.",
    "explanation": "Interpola linearmente tra due campioni e loro etichette: x = λx₁ + (1-λ)x₂. Migliora generalizzazione e calibrazione. Regolarizzazione semplice ma efficace.",
    "examples": ["Augmentation classificazione immagini", "Tecnica regolarizzazione", "Generalizzazione migliorata"],
    "relatedTerms": ["data-augmentation", "regularization", "cutmix"]
  },
  {
    "slug": "artificial-intelligence",
    "language": "en",
    "term": "Artificial Intelligence (AI)",
    "category": "Concept",
    "definition": "Field of computer science focused on creating systems capable of performing tasks requiring human intelligence.",
    "explanation": "AI encompasses machine learning, deep learning, NLP, computer vision. Goal is to create machines that can reason, learn, perceive, and make decisions.",
    "examples": ["Virtual assistants", "Self-driving cars", "Recommendation systems"],
    "relatedTerms": ["machine-learning", "deep-learning", "intelligence"]
  },
  {
    "slug": "artificial-intelligence-it",
    "language": "it",
    "term": "Intelligenza Artificiale (AI)",
    "category": "Concetto",
    "definition": "Campo dell'informatica focalizzato sulla creazione di sistemi capaci di eseguire task che richiedono intelligenza umana.",
    "explanation": "L'AI comprende machine learning, deep learning, NLP, computer vision. Obiettivo è creare macchine che possano ragionare, apprendere, percepire e prendere decisioni.",
    "examples": ["Assistenti virtuali", "Auto a guida autonoma", "Sistemi di raccomandazione"],
    "relatedTerms": ["machine-learning", "deep-learning", "intelligence"]
  },
  {
    "slug": "vector",
    "language": "en",
    "term": "Vector",
    "category": "Concept",
    "definition": "An ordered array of numbers representing a point in multi-dimensional space.",
    "explanation": "Fundamental data structure in ML. Can represent features, embeddings, or any numerical data. Operations include dot product, norm, addition.",
    "examples": ["Feature vector [1.2, 3.4, 5.6]", "Word embedding vector", "Image flattened to vector"],
    "relatedTerms": ["tensor", "matrix", "embedding"]
  },
  {
    "slug": "vector-it",
    "language": "it",
    "term": "Vettore",
    "category": "Concetto",
    "definition": "Un array ordinato di numeri rappresentante un punto nello spazio multi-dimensionale.",
    "explanation": "Struttura dati fondamentale in ML. Può rappresentare feature, embedding o qualsiasi dato numerico. Operazioni includono prodotto scalare, norma, addizione.",
    "examples": ["Vettore feature [1.2, 3.4, 5.6]", "Vettore word embedding", "Immagine appiattita a vettore"],
    "relatedTerms": ["tensor", "matrix", "embedding"]
  },
  {
    "slug": "matrix",
    "language": "en",
    "term": "Matrix",
    "category": "Concept",
    "definition": "A 2D array of numbers arranged in rows and columns.",
    "explanation": "Used to represent data batches, weights in neural networks, transformations. Operations include multiplication, transpose, inverse.",
    "examples": ["Weight matrix in neural networks", "Batch of data samples", "Image as matrix"],
    "relatedTerms": ["vector", "tensor", "linear-algebra"]
  },
  {
    "slug": "matrix-it",
    "language": "it",
    "term": "Matrice",
    "category": "Concetto",
    "definition": "Un array 2D di numeri organizzati in righe e colonne.",
    "explanation": "Usata per rappresentare batch di dati, pesi nelle reti neurali, trasformazioni. Operazioni includono moltiplicazione, trasposta, inversa.",
    "examples": ["Matrice pesi nelle reti neurali", "Batch di campioni dati", "Immagine come matrice"],
    "relatedTerms": ["vector", "tensor", "linear-algebra"]
  },
  {
    "slug": "scalar",
    "language": "en",
    "term": "Scalar",
    "category": "Concept",
    "definition": "A single numerical value, a zero-dimensional tensor.",
    "explanation": "Simplest form of data. Examples: loss value, learning rate, temperature. Contrasts with vectors and matrices.",
    "examples": ["Learning rate: 0.001", "Loss value: 2.5", "Temperature: 0.7"],
    "relatedTerms": ["vector", "tensor", "number"]
  },
  {
    "slug": "scalar-it",
    "language": "it",
    "term": "Scalare",
    "category": "Concetto",
    "definition": "Un singolo valore numerico, un tensore zero-dimensionale.",
    "explanation": "Forma più semplice di dati. Esempi: valore loss, learning rate, temperature. Contrasta con vettori e matrici.",
    "examples": ["Learning rate: 0.001", "Valore loss: 2.5", "Temperature: 0.7"],
    "relatedTerms": ["vector", "tensor", "number"]
  },
  {
    "slug": "dataset",
    "language": "en",
    "term": "Dataset",
    "category": "Concept",
    "definition": "A collection of data examples used for training, validation, or testing machine learning models.",
    "explanation": "Split into train/validation/test sets. Quality and size crucial for model performance. Can be labeled or unlabeled.",
    "examples": ["ImageNet dataset", "Training set of 1000 images", "MNIST handwritten digits"],
    "relatedTerms": ["training-data", "validation-set", "test-set"]
  },
  {
    "slug": "dataset-it",
    "language": "it",
    "term": "Dataset",
    "category": "Concetto",
    "definition": "Una collezione di esempi di dati usati per training, validation o testing di modelli machine learning.",
    "explanation": "Diviso in set train/validation/test. Qualità e dimensione cruciali per performance modello. Può essere etichettato o non etichettato.",
    "examples": ["Dataset ImageNet", "Training set di 1000 immagini", "Cifre scritte a mano MNIST"],
    "relatedTerms": ["training-data", "validation-set", "test-set"]
  },
  {
    "slug": "label",
    "language": "en",
    "term": "Label",
    "category": "Concept",
    "definition": "The target output or ground truth associated with a training example in supervised learning.",
    "explanation": "Labels indicate the correct answer. In classification: class names. In regression: numerical values. Required for supervised learning.",
    "examples": ["Image label: 'cat'", "Spam label: 0 or 1", "House price: $250,000"],
    "relatedTerms": ["supervised-learning", "ground-truth", "annotation"]
  },
  {
    "slug": "label-it",
    "language": "it",
    "term": "Etichetta",
    "category": "Concetto",
    "definition": "L'output target o ground truth associato a un esempio di training nell'apprendimento supervisionato.",
    "explanation": "Le etichette indicano la risposta corretta. Nella classificazione: nomi classi. Nella regressione: valori numerici. Richieste per apprendimento supervisionato.",
    "examples": ["Etichetta immagine: 'gatto'", "Etichetta spam: 0 o 1", "Prezzo casa: €250.000"],
    "relatedTerms": ["supervised-learning", "ground-truth", "annotation"]
  },
  {
    "slug": "feature",
    "language": "en",
    "term": "Feature",
    "category": "Concept",
    "definition": "An individual measurable property or characteristic of data used as input to a model.",
    "explanation": "Features represent data attributes. Can be raw (pixel values) or engineered (age, income). Quality of features impacts model performance.",
    "examples": ["Image pixels as features", "Age and income features", "Word count in text"],
    "relatedTerms": ["feature-engineering", "input", "attribute"]
  },
  {
    "slug": "feature-it",
    "language": "it",
    "term": "Feature",
    "category": "Concetto",
    "definition": "Una proprietà o caratteristica misurabile individuale dei dati usata come input a un modello.",
    "explanation": "Le feature rappresentano attributi dei dati. Possono essere grezze (valori pixel) o ingegnerizzate (età, reddito). Qualità delle feature impatta performance modello.",
    "examples": ["Pixel immagine come feature", "Feature età e reddito", "Conteggio parole nel testo"],
    "relatedTerms": ["feature-engineering", "input", "attribute"]
  },
  {
    "slug": "model",
    "language": "en",
    "term": "Model",
    "category": "Concept",
    "definition": "A mathematical representation learned from data that makes predictions or decisions.",
    "explanation": "Models learn patterns from training data. Can be neural networks, decision trees, linear models. Evaluated on test data.",
    "examples": ["Neural network model", "Trained classifier", "Regression model"],
    "relatedTerms": ["training", "prediction", "algorithm"]
  },
  {
    "slug": "model-it",
    "language": "it",
    "term": "Modello",
    "category": "Concetto",
    "definition": "Una rappresentazione matematica appresa dai dati che fa predizioni o decisioni.",
    "explanation": "I modelli apprendono pattern dai dati di training. Possono essere reti neurali, decision tree, modelli lineari. Valutati su dati test.",
    "examples": ["Modello rete neurale", "Classificatore trained", "Modello regressione"],
    "relatedTerms": ["training", "prediction", "algorithm"]
  },
  {
    "slug": "prediction",
    "language": "en",
    "term": "Prediction",
    "category": "Concept",
    "definition": "The output produced by a trained model when given new input data.",
    "explanation": "Models make predictions on unseen data. Accuracy compared to ground truth measures performance. Also called inference.",
    "examples": ["Predicting 'dog' for image", "Forecasting stock price", "Classifying email as spam"],
    "relatedTerms": ["inference", "output", "classification"]
  },
  {
    "slug": "prediction-it",
    "language": "it",
    "term": "Predizione",
    "category": "Concetto",
    "definition": "L'output prodotto da un modello trained quando riceve nuovi dati di input.",
    "explanation": "I modelli fanno predizioni su dati non visti. Accuracy comparata a ground truth misura performance. Chiamata anche inference.",
    "examples": ["Predire 'cane' per immagine", "Prevedere prezzo azioni", "Classificare email come spam"],
    "relatedTerms": ["inference", "output", "classification"]
  },
  {
    "slug": "inference",
    "language": "en",
    "term": "Inference",
    "category": "Concept",
    "definition": "The process of using a trained model to make predictions on new data.",
    "explanation": "Inference happens after training. Model processes input and generates output. Can be real-time or batch. Optimized for speed.",
    "examples": ["Real-time image classification", "Batch prediction on 1000 samples", "Model deployment"],
    "relatedTerms": ["prediction", "deployment", "serving"]
  },
  {
    "slug": "inference-it",
    "language": "it",
    "term": "Inference",
    "category": "Concetto",
    "definition": "Il processo di usare un modello trained per fare predizioni su nuovi dati.",
    "explanation": "L'inference avviene dopo il training. Il modello processa input e genera output. Può essere real-time o batch. Ottimizzato per velocità.",
    "examples": ["Classificazione immagini real-time", "Predizione batch su 1000 campioni", "Deployment modello"],
    "relatedTerms": ["prediction", "deployment", "serving"]
  },
  {
    "slug": "neuron",
    "language": "en",
    "term": "Neuron",
    "category": "Concept",
    "definition": "Basic computational unit in neural networks that receives inputs, applies weights and activation, produces output.",
    "explanation": "Inspired by biological neurons. Computes weighted sum of inputs plus bias, then applies activation function. Building block of neural networks.",
    "examples": ["Single neuron in hidden layer", "Output neuron", "Perceptron"],
    "relatedTerms": ["neural-network", "activation-function", "perceptron"]
  },
  {
    "slug": "neuron-it",
    "language": "it",
    "term": "Neurone",
    "category": "Concetto",
    "definition": "Unità computazionale base nelle reti neurali che riceve input, applica pesi e attivazione, produce output.",
    "explanation": "Ispirato dai neuroni biologici. Calcola somma pesata degli input più bias, poi applica funzione di attivazione. Building block delle reti neurali.",
    "examples": ["Singolo neurone in hidden layer", "Neurone output", "Perceptron"],
    "relatedTerms": ["neural-network", "activation-function", "perceptron"]
  },
  {
    "slug": "weight",
    "language": "en",
    "term": "Weight",
    "category": "Concept",
    "definition": "Learnable parameters in neural networks that determine the strength of connections between neurons.",
    "explanation": "Weights are adjusted during training to minimize loss. Multiplied with inputs in each neuron. Core of what model learns.",
    "examples": ["Connection weight: 0.5", "Weight matrix", "Learned parameters"],
    "relatedTerms": ["parameter", "training", "neural-network"]
  },
  {
    "slug": "weight-it",
    "language": "it",
    "term": "Peso",
    "category": "Concetto",
    "definition": "Parametri apprendibili nelle reti neurali che determinano la forza delle connessioni tra neuroni.",
    "explanation": "I pesi sono aggiustati durante il training per minimizzare la loss. Moltiplicati con input in ogni neurone. Core di ciò che il modello apprende.",
    "examples": ["Peso connessione: 0.5", "Matrice pesi", "Parametri appresi"],
    "relatedTerms": ["parameter", "training", "neural-network"]
  },
  {
    "slug": "parameter",
    "language": "en",
    "term": "Parameter",
    "category": "Concept",
    "definition": "Learnable values in a model that are optimized during training (weights and biases).",
    "explanation": "Parameters define the model. Number of parameters indicates model capacity. Large models have billions of parameters.",
    "examples": ["GPT-3: 175B parameters", "Weight and bias parameters", "Model size"],
    "relatedTerms": ["weight", "bias-term", "model-size"]
  },
  {
    "slug": "parameter-it",
    "language": "it",
    "term": "Parametro",
    "category": "Concetto",
    "definition": "Valori apprendibili in un modello che sono ottimizzati durante il training (pesi e bias).",
    "explanation": "I parametri definiscono il modello. Numero di parametri indica capacità modello. Modelli grandi hanno miliardi di parametri.",
    "examples": ["GPT-3: 175B parametri", "Parametri peso e bias", "Dimensione modello"],
    "relatedTerms": ["weight", "bias-term", "model-size"]
  },
  {
    "slug": "algorithm",
    "language": "en",
    "term": "Algorithm",
    "category": "Concept",
    "definition": "A step-by-step procedure or formula for solving a problem or performing a task.",
    "explanation": "In ML: learning algorithms (gradient descent), optimization algorithms, search algorithms. Defines how model learns or makes decisions.",
    "examples": ["Gradient descent algorithm", "K-means clustering", "Backpropagation"],
    "relatedTerms": ["gradient-descent", "optimization", "procedure"]
  },
  {
    "slug": "algorithm-it",
    "language": "it",
    "term": "Algoritmo",
    "category": "Concetto",
    "definition": "Una procedura o formula step-by-step per risolvere un problema o eseguire un task.",
    "explanation": "In ML: algoritmi di apprendimento (gradient descent), algoritmi di ottimizzazione, algoritmi di ricerca. Definisce come il modello apprende o prende decisioni.",
    "examples": ["Algoritmo gradient descent", "K-means clustering", "Backpropagation"],
    "relatedTerms": ["gradient-descent", "optimization", "procedure"]
  },
  {
    "slug": "error",
    "language": "en",
    "term": "Error",
    "category": "Concept",
    "definition": "The difference between predicted output and true label, indicating model's mistakes.",
    "explanation": "Error guides learning. High error means poor predictions. Training aims to minimize error. Related to loss function.",
    "examples": ["Prediction error: 0.5", "Classification error rate", "Mean squared error"],
    "relatedTerms": ["loss-function", "residual", "mistake"]
  },
  {
    "slug": "error-it",
    "language": "it",
    "term": "Errore",
    "category": "Concetto",
    "definition": "La differenza tra output predetto e etichetta vera, indicando errori del modello.",
    "explanation": "L'errore guida l'apprendimento. Errore alto significa predizioni scarse. Il training mira a minimizzare l'errore. Correlato alla funzione di loss.",
    "examples": ["Errore predizione: 0.5", "Tasso errore classificazione", "Mean squared error"],
    "relatedTerms": ["loss-function", "residual", "mistake"]
  }
]
